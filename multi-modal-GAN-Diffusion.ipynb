{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826871b2-b4c2-4f06-948f-1fc15fb62288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3c6071-6207-4aec-ae90-c48feeaebbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005af1d6-17ab-478f-afef-3f894a3f95f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os.path\n",
    "from PIL import Image\n",
    "from skimage.io import imread, imsave, imshow, show, imread_collection, imshow_collection\n",
    "import os, logging\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'  \n",
    "from os import listdir\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "#import keras_tuner as kt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tensorflow import keras\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "os.environ[\"TF_CPP_VMODULE\"]=\"gpu_process_state=10,gpu_cudamallocasync_allocator=10\"\n",
    "a = tf.zeros([], tf.float32)\n",
    "\n",
    "## Imports libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e7aef-3df9-40d4-836d-b126c784be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import clear_output\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd700ed5-e397-4ff9-b5e8-ff346ea89934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renameImageFiles(folderpath, prefix,fileExtension):\n",
    "    folder_path = folderpath\n",
    "    new_prefix = prefix\n",
    "\n",
    "    for i, file_path in enumerate(glob.glob(folder_path + '*.'+fileExtension)):\n",
    "        new_file_name = new_prefix + '_' + str(i+1) + '.'+fileExtension\n",
    "        os.rename(file_path, os.path.join(folder_path, new_file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8139e06-0ac5-4a2f-8751-3b023fa16095",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43d606-28a3-4529-bf31-ba5a48804f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/home/maxwellsam/Multimodal-COVID19-Images/dataset/CT_COVID/'\n",
    "prefix1 = 'ct_covid'\n",
    "path2 = '/home/maxwellsam/Multimodal-COVID19-Images/dataset/CT_NonCOVID/'\n",
    "prefix2 = 'ct_noncovid'\n",
    "\n",
    "# renameImageFiles(path1, prefix1,'png')\n",
    "# renameImageFiles(path2, prefix2,'png')\n",
    "# renameImageFiles(path2, prefix2,'jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c16b27-c2cb-46a6-ab18-88eebdc78c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "path3 = '/home/maxwellsam/Multimodal-COVID19-Images/dataset/Shap/CT_COVID/'\n",
    "prefix3 = 'ct_covid'\n",
    "path4 = '/home/maxwellsam/Multimodal-COVID19-Images/dataset/Shap/CT_NonCOVID/'\n",
    "prefix4 = 'ct_noncovid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b3c26-6676-47f5-92e7-4028af48b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImages(imgDirPath, binary_label):\n",
    "    img_names = list()\n",
    "    try:\n",
    "        with os.scandir(imgDirPath) as dirs:\n",
    "            for entry in dirs:\n",
    "                img_names.append(entry.name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scanning directory: {e}\")\n",
    "        return None\n",
    "\n",
    "    all_features = []\n",
    "    for img in img_names:\n",
    "        try:\n",
    "            path = imgDirPath + img\n",
    "            cv_img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if cv_img is None:\n",
    "                print(f\"Error reading image: {path}\")\n",
    "                continue\n",
    "\n",
    "            cv_img2 = cv2.resize(cv_img, (300, 300), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            nFeatures = (cv_img2.shape[0] * cv_img2.shape[1])\n",
    "            features = np.reshape(cv_img2, nFeatures)\n",
    "            all_features.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img}: {e}\")\n",
    "\n",
    "    if len(all_features) == 0:\n",
    "        print(\"No valid images found.\")\n",
    "        return None\n",
    "\n",
    "    imgs_df = pd.DataFrame(np.array(all_features), index=img_names)\n",
    "    if binary_label == 0:\n",
    "        imgs_df['class_label'] = np.zeros((imgs_df.shape[0]), dtype=int)\n",
    "    else:\n",
    "        imgs_df['class_label'] = np.ones((imgs_df.shape[0]), dtype=int)\n",
    "\n",
    "    return imgs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c57e34-4ffb-44de-b31c-7d17da669ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df =  processImages(path1,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df =  processImages(path2,0)#0 ---> covnid-19 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301cba6-be1a-4098-a1e8-d888515b4eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df_shap =  processImages(path3,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df_shap =  processImages(path4,0)#0 ---> covnid-19 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76129c40-46be-4cdc-9d7d-05b7c271dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs = [ct_noncovid_features_df, ct_covid_features_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset = cvd_imgs_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db2820-97c9-4eaa-8278-b66835afa21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_shap = [ct_noncovid_features_df_shap, ct_covid_features_df_shap]\n",
    "cvd_imgs_dataset_shap = pd.concat(cvd_imgs_shap)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_shap = cvd_imgs_dataset_shap.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_shap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da92581f-76e4-4814-9fdb-488a92333af8",
   "metadata": {},
   "source": [
    "#### Prepare negative covid images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c818e-d86d-470e-98b5-54158757cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x_ = ct_noncovid_features_df.iloc[:,:-1].to_numpy().reshape((2173,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "ct_noncovid_features_df['output_encode'] = label_encoder.fit_transform(ct_noncovid_features_df['class_label'])\n",
    "ct_noncovid_features_df\n",
    "ct_noncovid_features_df = pd.get_dummies(ct_noncovid_features_df, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y_ = np.array(ct_noncovid_features_df[['class_label']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "input_data_x_negative_only = input_data_x_\n",
    "output_label_y_negative_only = output_label_y_\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x_negative_only.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y_negative_only.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dd2e23-c04c-4b28-9905-1cf3a2f13632",
   "metadata": {},
   "source": [
    "#### Prepare positive covid images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd730d-d06a-4f4c-93c6-0f0e13dd06f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x__ = ct_covid_features_df.iloc[:,:-1].to_numpy().reshape((2476,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "ct_covid_features_df['output_encode'] = label_encoder.fit_transform(ct_covid_features_df['class_label'])\n",
    "ct_covid_features_df\n",
    "ct_covid_features_df = pd.get_dummies(ct_covid_features_df, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y__ = np.array(ct_covid_features_df[['class_label']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "input_data_x_positive_only = input_data_x__\n",
    "output_label_y_positive_only = output_label_y__\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x_positive_only.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y_positive_only.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40129d3-cfe2-40a7-a92e-7e9d7bcb648d",
   "metadata": {},
   "source": [
    "#### Prepare both positive and negetive covid images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf49cf6-245c-4a59-8057-7b4b085e48a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy().reshape((4649,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset['class_label'])\n",
    "cvd_imgs_dataset\n",
    "cvd_imgs_dataset = pd.get_dummies(cvd_imgs_dataset, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y = np.array(cvd_imgs_dataset[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ee1fee-5c3c-4c83-a2c5-a35c0355ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61627bba-aa64-4e87-a4d2-976474b0d446",
   "metadata": {},
   "source": [
    "#### Prepare both positive and negetive covid shap images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0168f1-b3c6-41d3-afc3-a8823850e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x_shap = cvd_imgs_dataset_shap.iloc[:,:-1].to_numpy().reshape((3718,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_shap['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_shap['class_label'])\n",
    "cvd_imgs_dataset_shap\n",
    "cvd_imgs_dataset_shap = pd.get_dummies(cvd_imgs_dataset_shap, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y_shap = np.array(cvd_imgs_dataset_shap[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x_shap.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y_shap.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b452f-e552-40ee-bdfe-893f5bdd2d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_dataset_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c34d31-c8e0-479f-af16-f4722620d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    input_data_x, output_label_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7896f533-3853-439a-ade0-5b4e89b9dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_shap, test_features_shap, train_labels_shap, test_labels_shap = train_test_split(\n",
    "    input_data_x_shap, output_label_y_shap, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdacb99e-831a-4a74-bd0e-20cb3c4913b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_negative_only, test_features_negative_only, train_labels_negative_only, test_labels_negative_only = train_test_split(\n",
    "    input_data_x_negative_only, output_label_y_negative_only, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a4d179-0a4c-4d45-9499-e0632d1fa665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_positive_only, test_features_positive_only, train_labels_positive_only, test_labels__positive_only = train_test_split(\n",
    "    input_data_x_positive_only, output_label_y_positive_only, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb3e2f0-c872-4c6c-a696-344eaf751a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_shap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8cdef3-365f-4081-b848-1649400c2f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe390163-fa8f-4705-9401-7a4cf140326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_features_positive_only[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e78193-fa63-4ec8-9198-bbaf59a9f4d2",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ba69bb-db28-48f2-9e41-fc5cebb0beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, features, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (array-like): Input image features (e.g., images as numpy arrays).\n",
    "            labels (array-like): Corresponding labels for the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Ensure the image has the correct shape\n",
    "        if image.ndim == 2:  # Grayscale image (height, width)\n",
    "            image = np.expand_dims(image, axis=-1)  # Convert to (height, width, 1)\n",
    "        elif image.ndim == 3 and image.shape[2] == 1:  # If the image is (height, width, 1)\n",
    "            image = np.squeeze(image, axis=-1)  # Convert to (height, width)\n",
    "\n",
    "        # Convert to PIL image\n",
    "        if isinstance(image, np.ndarray):\n",
    "            if image.ndim == 2:  # Grayscale\n",
    "                image = Image.fromarray(image)  # Convert from numpy array to PIL Image\n",
    "            elif image.ndim == 3:  # RGB\n",
    "                image = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82412520-e794-4585-b0ba-d9a481fbb5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Custom normalization for grayscale images (mean and std for single channel)\n",
    "# normalize_grayscale = transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     normalize_grayscale,  # Apply normalization only for grayscale images\n",
    "# ])\n",
    "\n",
    "# Use a conditional normalization based on the number of channels\n",
    "def dynamic_normalize(tensor):\n",
    "    \"\"\"\n",
    "    Normalize tensors dynamically based on the number of channels.\n",
    "    Args:\n",
    "        tensor: A PyTorch tensor representing the image.\n",
    "    Returns:\n",
    "        Normalized tensor.\n",
    "    \"\"\"\n",
    "    if tensor.shape[0] == 3:  # RGB Image\n",
    "        return transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(tensor)\n",
    "    elif tensor.shape[0] == 1:  # Grayscale Image\n",
    "        return transforms.Normalize(mean=[0.485], std=[0.229])(tensor)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported number of channels: {tensor.shape[0]}\")\n",
    "\n",
    "# Add this dynamic normalization into your transform chain:\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    dynamic_normalize,  # Apply normalization dynamically based on channels\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9418fcf5-f8ea-4538-8324-9f895f45ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = CustomImageDataset(features=train_features, labels=train_labels, transform=transform)\n",
    "# #test_dataset = CustomImageDataset(features=test_features, labels=test_labels, transform=transform)\n",
    "\n",
    "# # Create DataLoader instances\n",
    "# loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# #test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0faec4f-84f6-43f5-9f50-d2ff2c5e4832",
   "metadata": {},
   "source": [
    "### Load datset for NonCOVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c74894-d8d9-468c-a8d0-2264ab66f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_negative = CustomImageDataset(features=train_features_negative_only, labels=train_labels_negative_only, transform=transform)\n",
    "#test_dataset = CustomImageDataset(features=test_features, labels=test_labels, transform=transform)\n",
    "\n",
    "# Create DataLoader instances\n",
    "loader_negative = DataLoader(train_dataset_negative, batch_size=64, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd220a6-a04f-4f19-8337-a9cf6ffab399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfa8e980-38ae-4b89-b8f1-dd4c1938ea1f",
   "metadata": {},
   "source": [
    "### Load datset for COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00330b6-11f8-49e6-89af-fa7e505d31df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_positive = CustomImageDataset(features=train_features_positive_only, labels=train_labels_positive_only, transform=transform)\n",
    "#test_dataset = CustomImageDataset(features=test_features, labels=test_labels, transform=transform)\n",
    "\n",
    "# Create DataLoader instances\n",
    "loader_positive = DataLoader(train_dataset_positive, batch_size=64, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313064bd-0ba2-437a-b6b4-2c8537a9a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_model = DiffusionModel(input_size=300*300).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5535afce-3183-4623-bb44-0e775e325896",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f011e9-ad43-4c1a-a583-9f1b4e60b1b2",
   "metadata": {},
   "source": [
    "### Building the multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca88b0-b2f3-404e-8048-81a3b80fec45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size=300):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, 2, 1),  \n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.3),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.3),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.3)\n",
    "        )\n",
    "\n",
    "        dummy_input = torch.zeros(1, 1, input_size, input_size)\n",
    "        conv_output_size = self.conv(dummy_input).view(1, -1).size(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(conv_output_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "\n",
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, output_size=300):\n",
    "        super().__init__()\n",
    "        intermediate_size = output_size // 8  # Dividing by 8 to match the upscale steps\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256 * intermediate_size * intermediate_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # Output: (128, intermediate_size*2, intermediate_size*2)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),  # Output: (64, intermediate_size*4, intermediate_size*4)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 1, 4, 2, 1),  # Output: (1, intermediate_size*8, intermediate_size*8)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        intermediate_size = int((x.size(1) // 256) ** 0.5)\n",
    "        x = x.view(-1, 256, intermediate_size, intermediate_size)\n",
    "        out = self.conv(x)\n",
    "        \n",
    "        # If the output size is not exactly 300x300, we pad it\n",
    "        out = F.pad(out, (0, 4, 0, 4))  # Pad width and height by 4\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the Diffusion Model\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, input_size=300 * 300):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, noise_level):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.encoder(x)\n",
    "        noise = torch.randn_like(x) * noise_level\n",
    "        x = x + noise\n",
    "        x = self.decoder(x)\n",
    "        x = x.view(x.size(0), 1, 300, 300)\n",
    "        return x\n",
    "\n",
    "\n",
    "def gradient_penalty(discriminator, real_data, fake_data, device):\n",
    "    alpha = torch.rand(real_data.size(0), 1, 1, 1, device=device)\n",
    "    interpolates = alpha * real_data + (1 - alpha) * fake_data\n",
    "    interpolates.requires_grad_(True)\n",
    "    d_interpolates = discriminator(interpolates)\n",
    "    fake = torch.ones(d_interpolates.size(), device=device)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11430841-97f9-4c20-8a9a-670dc3fa8b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8823b7-7db1-46ce-9bd3-b521d6b908ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb42829-9138-4229-abdf-ec5c4eadf675",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(z_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3123cf6c-94b5-48bf-a050-6eab4f49ed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e5336e-076d-4209-b01c-0ba0267ad9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "import torch.nn.functional as F\n",
    "torch.cuda.empty_cache() \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 0.0002\n",
    "z_dim = 100\n",
    "batch_size = 4\n",
    "num_epochs = 10\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "# Initialize models\n",
    "disc = Discriminator().to(device)\n",
    "gen = Generator(z_dim).to(device)\n",
    "diffusion_model = DiffusionModel().to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "\n",
    "# Optimizers\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr, betas=betas)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr, betas=betas)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# TensorBoard\n",
    "writer_fake = SummaryWriter('runs/CNN_GAN_MNIST/fake')\n",
    "writer_real = SummaryWriter('runs/CNN_GAN_MNIST/real')\n",
    "writer_losses = SummaryWriter('runs/CNN_GAN_MNIST/losses')\n",
    "\n",
    "# Start TensorBoard\n",
    "%tensorboard --logdir=runs/CNN_GAN_MNIST/\n",
    "\n",
    "save_dir = \"/home/maxwellsam/Multimodal-COVID19-Images/generatedImages/COVID-19_/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "lambda_gp = 10  # Gradient penalty weight\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader_positive):\n",
    "        real = real.to(device)\n",
    "        batch_size = real.shape[0]\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = gen(torch.randn(batch_size, z_dim).to(device))\n",
    "        print(f\"Fake shape after modification: {fake.shape}\")\n",
    "\n",
    "\n",
    "        # Discriminator Loss with Gradient Penalty\n",
    "        disc_real = disc(real).view(-1)\n",
    "        disc_fake = disc(fake.detach()).view(-1)\n",
    "        gp = gradient_penalty(disc, real, fake, device)\n",
    "        lossD = -torch.mean(disc_real) + torch.mean(disc_fake) + lambda_gp * gp\n",
    "        \n",
    "        disc.zero_grad()\n",
    "        lossD.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        # Generator Loss\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = -torch.mean(output)\n",
    "\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader_positive)} \"\n",
    "                f\"Loss D: {lossD:.4f}, Loss G: {lossG:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(real, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\"MNIST Fake Images\", img_grid_fake, global_step=step)\n",
    "                writer_real.add_image(\"MNIST Real Images\", img_grid_real, global_step=step)\n",
    "\n",
    "                for i, fake_img in enumerate(fake):\n",
    "                    normalized_img = (fake_img + 1) / 2\n",
    "                    save_path = os.path.join(\n",
    "                        save_dir, f\"epoch_{epoch}_batch_{batch_idx}_image_{i}.png\"\n",
    "                    )\n",
    "                    torchvision.utils.save_image(normalized_img, save_path)\n",
    "\n",
    "                writer_losses.add_scalar(\"Discriminator Loss\", lossD, global_step=step)\n",
    "                writer_losses.add_scalar(\"Generator Loss\", lossG, global_step=step)\n",
    "\n",
    "                step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac5f5c-353a-473b-9035-c51356c80cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84adbfb-963d-4d38-9e9b-1283e7573cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
