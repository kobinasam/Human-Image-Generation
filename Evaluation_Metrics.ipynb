{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b1322-cd3b-47f0-9c34-e657baddeaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os.path\n",
    "from PIL import Image\n",
    "from skimage.io import imread, imsave, imshow, show, imread_collection, imshow_collection\n",
    "import os, logging\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'  \n",
    "from os import listdir\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "#import keras_tuner as kt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tensorflow import keras\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "os.environ[\"TF_CPP_VMODULE\"]=\"gpu_process_state=10,gpu_cudamallocasync_allocator=10\"\n",
    "a = tf.zeros([], tf.float32)\n",
    "\n",
    "## Imports libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0adaccb-a80c-4c3d-8d4b-3c058ea146d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee50c0-b7d7-4737-a45c-c4709607e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import clear_output\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4671459-873f-4dbb-ac32-fb60e8b2d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renameImageFiles(folderpath, prefix,fileExtension):\n",
    "    folder_path = folderpath\n",
    "    new_prefix = prefix\n",
    "\n",
    "    for i, file_path in enumerate(glob.glob(folder_path + '*.'+fileExtension)):\n",
    "        new_file_name = new_prefix + '_' + str(i+1) + '.'+fileExtension\n",
    "        os.rename(file_path, os.path.join(folder_path, new_file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202de780-5fc3-41bb-a1f0-da676a7a751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b869cb92-b6a7-4eef-a2dc-e0725bc50897",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/home/maxwellsam/Multimodal-COVID19-Images/dataset/CT_COVID/'\n",
    "prefix1 = 'ct_covid'\n",
    "path2 = '/home/maxwellsam/Multimodal-COVID19-Images/dataset/CT_NonCOVID/'\n",
    "prefix2 = 'ct_noncovid'\n",
    "\n",
    "# renameImageFiles(path1, prefix1,'png')\n",
    "# renameImageFiles(path2, prefix2,'png')\n",
    "# renameImageFiles(path2, prefix2,'jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c447b4-73ca-452e-b504-a57902fb0bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path3 = '/home/maxwellsam/Multimodal-COVID19-Images/generatedImages/VAE/COVID-19/'\n",
    "prefix3 = 'ct_covid'\n",
    "path4 = \"/home/maxwellsam/Multimodal-COVID19-Images/generatedImages/VAE/Non-COVID-19/\"\n",
    "prefix4 = 'ct_noncovid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826fe22-0b47-44d1-ac36-e8f04246327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImages(imgDirPath, binary_label):\n",
    "    img_names = list()\n",
    "    try:\n",
    "        with os.scandir(imgDirPath) as dirs:\n",
    "            for entry in dirs:\n",
    "                img_names.append(entry.name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scanning directory: {e}\")\n",
    "        return None\n",
    "\n",
    "    all_features = []\n",
    "    for img in img_names:\n",
    "        try:\n",
    "            path = imgDirPath + img\n",
    "            cv_img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if cv_img is None:\n",
    "                print(f\"Error reading image: {path}\")\n",
    "                continue\n",
    "\n",
    "            cv_img2 = cv2.resize(cv_img, (300, 300), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            nFeatures = (cv_img2.shape[0] * cv_img2.shape[1])\n",
    "            features = np.reshape(cv_img2, nFeatures)\n",
    "            all_features.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img}: {e}\")\n",
    "\n",
    "    if len(all_features) == 0:\n",
    "        print(\"No valid images found.\")\n",
    "        return None\n",
    "\n",
    "    imgs_df = pd.DataFrame(np.array(all_features), index=img_names)\n",
    "    if binary_label == 0:\n",
    "        imgs_df['class_label'] = np.zeros((imgs_df.shape[0]), dtype=int)\n",
    "    else:\n",
    "        imgs_df['class_label'] = np.ones((imgs_df.shape[0]), dtype=int)\n",
    "\n",
    "    return imgs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ba0da-9d4c-43e6-a3ed-93d87be89df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51bd7d9-c2e1-43a8-b63d-0e011eaa6484",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df =  processImages(path1,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df =  processImages(path2,0)#0 ---> covid-19 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95819bb9-1e3b-48fc-b3cd-60efc83f2d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df_gen =  processImages(path3,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df_gen =  processImages(path4,0)#0 ---> covnid-19 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a48c23-f81b-43ea-9efc-41cac0b3f8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs = [ct_noncovid_features_df, ct_covid_features_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset = cvd_imgs_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84612014-462f-46eb-a0ad-464fe6b2f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cvd_imgs_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac7f07d-9f6f-448a-bd7e-e318f59e68b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_gen = [ct_noncovid_features_df_gen, ct_covid_features_df_gen]\n",
    "cvd_imgs_dataset_gen = pd.concat(cvd_imgs_gen)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_gen = cvd_imgs_dataset_gen.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbecf73a-6b0a-4101-a2f7-4025a445a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images = cvd_imgs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40b517d-3e20-42a1-a94f-b10d002686e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = cvd_imgs_dataset_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad5bd2-d7d9-4fdb-9587-f8a5df116321",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(generated_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bdcdcd-8247-43d3-b82a-6526ed438a37",
   "metadata": {},
   "source": [
    "#### Convert Imges to Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bc3987-1832-4323-ae67-8c610d46e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images_numpy = original_images.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1fec7-513f-4e51-99ca-88135744cdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images_numpy = generated_images.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c75dff-a618-4dcf-a25f-ecdb4ccf08d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images_numpy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bcad5f-f91d-4fd1-92f4-57f0787debdd",
   "metadata": {},
   "source": [
    "#### Convert Imges to Torch Tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41147a-9961-4370-9278-0abcacb734d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images_tensors = torch.from_numpy(original_images_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3147470-0817-4e12-bd80-6ff9f3abe667",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images_tensors = torch.from_numpy(generated_images_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a460ed49-45fa-4d62-b14c-d97be715bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images_tensors[:4649].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c1de5a-8c9e-48c2-b63d-ada24fc40f16",
   "metadata": {},
   "source": [
    "#### Calculating the Inception Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a630dbc-f152-44ef-91c1-5905ea0f6ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import inception_v3\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def inception_score(images, splits=10, batch_size=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Ensure images are PyTorch tensors\n",
    "    if isinstance(images, np.ndarray):\n",
    "        images = torch.from_numpy(images)\n",
    "\n",
    "    # Debugging: Print shape before processing\n",
    "    print(f\"Before Processing: {images.shape}\")\n",
    "\n",
    "    # Fix incorrectly flattened images\n",
    "    if len(images.shape) == 2:  # Shape (N, Flattened)\n",
    "        N, D = images.shape\n",
    "        h_w = int(D ** 0.5)\n",
    "        images = images.view(N, 1, h_w, h_w)  # Assume grayscale\n",
    "\n",
    "    # Ensure images have (N, C, H, W) format\n",
    "    if images.dim() != 4:\n",
    "        raise ValueError(f\"Images must have 4 dimensions (N, C, H, W), but got {images.shape}\")\n",
    "\n",
    "    # Convert grayscale (1 channel) to RGB\n",
    "    if images.shape[1] == 1:\n",
    "        images = images.repeat(1, 3, 1, 1)  # Convert (N, 1, H, W) → (N, 3, H, W)\n",
    "\n",
    "    # Normalize images\n",
    "    images = images.float()\n",
    "    if images.max() > 1:\n",
    "        images /= 255.0\n",
    "\n",
    "    # Resize to 299x299 for Inception v3\n",
    "    images = F.interpolate(images, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "\n",
    "    # Move to device\n",
    "    images = images.to(device)\n",
    "    model = inception_v3(pretrained=True, transform_input=False).eval().to(device)\n",
    "\n",
    "    scores = []\n",
    "    num_batches = (len(images) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch = images[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = F.softmax(model(batch), dim=1).cpu().numpy()\n",
    "\n",
    "        # Compute Inception Score for batch\n",
    "        split_scores = []\n",
    "        for k in range(splits):\n",
    "            part = preds[k * (len(preds) // splits): (k + 1) * (len(preds) // splits), :]\n",
    "            py = np.mean(part, axis=0)\n",
    "            scores_batch = entropy(part.T, py[:, None], base=np.e)\n",
    "            split_scores.append(np.exp(np.mean(scores_batch)))\n",
    "\n",
    "        scores.append(np.mean(split_scores))\n",
    "\n",
    "    return np.mean(scores), np.std(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b6942a-5854-436a-9b64-9a28f58749bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7855d01c-fc14-4f2a-9740-4aeff9ed47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gen_images = generated_images.iloc[:,:-1].to_numpy().reshape((400,300,300,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c18d088-610a-4c56-938d-084985def14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10  # Adjust this based on your GPU capacity\n",
    "num_batches = (len(my_gen_images) + batch_size - 1) // batch_size  # Compute number of batches\n",
    "\n",
    "scores = []\n",
    "for i in range(num_batches):\n",
    "    batch = my_gen_images[i * batch_size: (i + 1) * batch_size]\n",
    "    score, _ = inception_score(batch)\n",
    "    scores.append(score)\n",
    "\n",
    "final_score = np.mean(scores)\n",
    "print(f\"Final Inception Score: {final_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbf073-01a0-4a5e-97a0-ff8530f5dd29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5c78b92-4f20-4736-aad2-54186215e0cf",
   "metadata": {},
   "source": [
    "#### Structural Similarity Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ffb91-22db-446d-8098-fd3f5897c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    ssim_value, _ = ssim(img1, img2, full=True, multichannel=True)\n",
    "    return ssim_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26ef41-386a-4dfb-ac4f-6eaaf5721c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_ssim(original_images_numpy[:400], generated_images_numpy[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa0f8a-30a6-4860-b010-8afe2146585f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dfeaaf8-893a-4866-b2d0-f8fc9db86edb",
   "metadata": {},
   "source": [
    "#### Fréchet Inception Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2050367-a796-4812-82e8-ca39102ff745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.inception import inception_v3\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def calculate_fid(real_images, fake_images):\n",
    "    model = inception_v3(pretrained=True, transform_input=False).eval().cuda()\n",
    "    real_features = get_features(model, real_images)\n",
    "    fake_features = get_features(model, fake_images)\n",
    "\n",
    "    mu_real, sigma_real = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu_fake, sigma_fake = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)\n",
    "    ssdiff = np.sum((mu_real - mu_fake) ** 2)\n",
    "    covmean = sqrtm(sigma_real.dot(sigma_fake))\n",
    "\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    fid = ssdiff + np.trace(sigma_real + sigma_fake - 2 * covmean)\n",
    "    return fid\n",
    "\n",
    "def get_features(model, images):\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for img in images:\n",
    "            img = img.unsqueeze(0).cuda()\n",
    "            feature = model(img).squeeze().cpu().numpy()\n",
    "            features.append(feature)\n",
    "    return np.array(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3229958e-5cde-4f09-b8ac-f71c4fb8e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image_FID = cvd_imgs_dataset.iloc[:,:-1].to_numpy().reshape((4649,300,300,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b4601f-5350-4ef4-859e-a4514a2f95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image_tensor_FID = torch.from_numpy(original_image_FID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640a3ba9-802f-40e1-b2a7-f2e20c9d8612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Assume my_gen_tensor__ has shape [14012, 300, 300, 1]\n",
    "# Permute to shape [14012, 1, 300, 300]\n",
    "images_ = original_image_tensor_FID.permute(0, 3, 1, 2)\n",
    "\n",
    "# Now pass the images through the interpolation function\n",
    "images_resized_ = F.interpolate(images_, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "\n",
    "# Check the new shape\n",
    "print(\"Resized images shape:\", images_resized_.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce145ea0-cebf-47f4-81bd-092083a5fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def convert_to_rgb(images):\n",
    "    return images.repeat(1, 3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aced899-ec65-4e01-a03c-a32e26144d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images_rgb = convert_to_rgb(original_image_tensor_FID[:10])\n",
    "fake_images_rgb = convert_to_rgb(images_resized[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fdb540-4157-4147-b541-55a18ceb6e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(fake_images_rgb.float().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a474fda6-79e3-4e90-927a-8521b93da23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_fid(real_images_rgb.float().cuda(), fake_images_rgb.float().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8266092-2045-4f1c-b69a-ca3249a8f124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d3aedaa-b3eb-4890-9977-6647375fbafb",
   "metadata": {},
   "source": [
    "#### Learned Perceptual Image Patch Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c049d-5655-4fc0-96e4-d0b8e0b03682",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e02195-84bd-4dd2-89a7-7d350746f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lpips\n",
    "import torch\n",
    "\n",
    "def calculate_lpips(img1, img2):\n",
    "    loss_fn = lpips.LPIPS(net='vgg')  \n",
    "\n",
    "    img1 = img1[:, :-1].view(-1, 1, 300, 300) \n",
    "    img2 = img2[:, :-1].view(-1, 1, 300, 300)\n",
    "    img1 = img1.repeat(1, 3, 1, 1)  \n",
    "    img2 = img2.repeat(1, 3, 1, 1)\n",
    "\n",
    "    lpips_value = loss_fn(img1, img2)\n",
    "    return lpips_value.mean().item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2f1903-ea83-4d77-8b57-aed727c78c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_lpips(original_images_tensors[:100], generated_images_tensors[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c64fba8-8c18-4414-9440-af2dd62313df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
