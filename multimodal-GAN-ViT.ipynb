{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826871b2-b4c2-4f06-948f-1fc15fb62288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3c6071-6207-4aec-ae90-c48feeaebbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005af1d6-17ab-478f-afef-3f894a3f95f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os.path\n",
    "from PIL import Image\n",
    "from skimage.io import imread, imsave, imshow, show, imread_collection, imshow_collection\n",
    "import os, logging\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'  \n",
    "from os import listdir\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "#import keras_tuner as kt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tensorflow import keras\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "os.environ[\"TF_CPP_VMODULE\"]=\"gpu_process_state=10,gpu_cudamallocasync_allocator=10\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "a = tf.zeros([], tf.float32)\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "## Imports libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e7aef-3df9-40d4-836d-b126c784be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import clear_output\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd700ed5-e397-4ff9-b5e8-ff346ea89934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renameImageFiles(folderpath, prefix,fileExtension):\n",
    "    folder_path = folderpath\n",
    "    new_prefix = prefix\n",
    "\n",
    "    for i, file_path in enumerate(glob.glob(folder_path + '*.'+fileExtension)):\n",
    "        new_file_name = new_prefix + '_' + str(i+1) + '.'+fileExtension\n",
    "        os.rename(file_path, os.path.join(folder_path, new_file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8139e06-0ac5-4a2f-8751-3b023fa16095",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43d606-28a3-4529-bf31-ba5a48804f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/home/maxwellsam/Multimodal-COVID19-Images/dataset/CT_COVID/'\n",
    "prefix1 = 'ct_covid'\n",
    "path2 = '/home/maxwellsam/Multimodal-COVID19-Images/dataset/CT_NonCOVID/'\n",
    "prefix2 = 'ct_noncovid'\n",
    "\n",
    "# renameImageFiles(path1, prefix1,'png')\n",
    "# renameImageFiles(path2, prefix2,'png')\n",
    "# renameImageFiles(path2, prefix2,'jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c16b27-c2cb-46a6-ab18-88eebdc78c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "path3 = '/home/maxwellsam/Multimodal-COVID19-Images/dataset/Shap/CT_COVID/'\n",
    "prefix3 = 'ct_covid'\n",
    "path4 = '/home/maxwellsam/Multimodal-COVID19-Images/dataset/Shap/CT_NonCOVID/'\n",
    "prefix4 = 'ct_noncovid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b3c26-6676-47f5-92e7-4028af48b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImages(imgDirPath, binary_label):\n",
    "    img_names = list()\n",
    "    try:\n",
    "        with os.scandir(imgDirPath) as dirs:\n",
    "            for entry in dirs:\n",
    "                img_names.append(entry.name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scanning directory: {e}\")\n",
    "        return None\n",
    "\n",
    "    all_features = []\n",
    "    for img in img_names:\n",
    "        try:\n",
    "            path = imgDirPath + img\n",
    "            cv_img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if cv_img is None:\n",
    "                print(f\"Error reading image: {path}\")\n",
    "                continue\n",
    "\n",
    "            cv_img2 = cv2.resize(cv_img, (300, 300), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            nFeatures = (cv_img2.shape[0] * cv_img2.shape[1])\n",
    "            features = np.reshape(cv_img2, nFeatures)\n",
    "            all_features.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img}: {e}\")\n",
    "\n",
    "    if len(all_features) == 0:\n",
    "        print(\"No valid images found.\")\n",
    "        return None\n",
    "\n",
    "    imgs_df = pd.DataFrame(np.array(all_features), index=img_names)\n",
    "    if binary_label == 0:\n",
    "        imgs_df['class_label'] = np.zeros((imgs_df.shape[0]), dtype=int)\n",
    "    else:\n",
    "        imgs_df['class_label'] = np.ones((imgs_df.shape[0]), dtype=int)\n",
    "\n",
    "    return imgs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c57e34-4ffb-44de-b31c-7d17da669ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df =  processImages(path1,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df =  processImages(path2,0)#0 ---> covid-19 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301cba6-be1a-4098-a1e8-d888515b4eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df_shap =  processImages(path3,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df_shap =  processImages(path4,0)#0 ---> covnid-19 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76129c40-46be-4cdc-9d7d-05b7c271dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs = [ct_noncovid_features_df, ct_covid_features_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset = cvd_imgs_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db2820-97c9-4eaa-8278-b66835afa21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_shap = [ct_noncovid_features_df_shap, ct_covid_features_df_shap]\n",
    "cvd_imgs_dataset_shap = pd.concat(cvd_imgs_shap)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_shap = cvd_imgs_dataset_shap.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_shap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da92581f-76e4-4814-9fdb-488a92333af8",
   "metadata": {},
   "source": [
    "#### Prepare negative covid images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c818e-d86d-470e-98b5-54158757cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x_ = ct_noncovid_features_df.iloc[:,:-1].to_numpy().reshape((2173,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "ct_noncovid_features_df['output_encode'] = label_encoder.fit_transform(ct_noncovid_features_df['class_label'])\n",
    "ct_noncovid_features_df\n",
    "ct_noncovid_features_df = pd.get_dummies(ct_noncovid_features_df, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y_ = np.array(ct_noncovid_features_df[['class_label']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "input_data_x_negative_only = input_data_x_\n",
    "output_label_y_negative_only = output_label_y_\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x_negative_only.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y_negative_only.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dd2e23-c04c-4b28-9905-1cf3a2f13632",
   "metadata": {},
   "source": [
    "#### Prepare positive covid images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd730d-d06a-4f4c-93c6-0f0e13dd06f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x__ = ct_covid_features_df.iloc[:,:-1].to_numpy().reshape((2476,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "ct_covid_features_df['output_encode'] = label_encoder.fit_transform(ct_covid_features_df['class_label'])\n",
    "ct_covid_features_df\n",
    "ct_covid_features_df = pd.get_dummies(ct_covid_features_df, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y__ = np.array(ct_covid_features_df[['class_label']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "input_data_x_positive_only = input_data_x__\n",
    "output_label_y_positive_only = output_label_y__\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x_positive_only.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y_positive_only.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40129d3-cfe2-40a7-a92e-7e9d7bcb648d",
   "metadata": {},
   "source": [
    "#### Prepare both positive and negetive covid images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf49cf6-245c-4a59-8057-7b4b085e48a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy().reshape((4649,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset['class_label'])\n",
    "cvd_imgs_dataset\n",
    "cvd_imgs_dataset = pd.get_dummies(cvd_imgs_dataset, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y = np.array(cvd_imgs_dataset[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ee1fee-5c3c-4c83-a2c5-a35c0355ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e7d8bf-ac29-4ab0-a149-d6b6a1d01caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df_shap.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dd9e7e-8961-4670-b74e-e320158a979f",
   "metadata": {},
   "source": [
    "#### Prepare negative covid shap-images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e530a-ef63-4ce4-8ea1-42e7d3d117c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x_shap = ct_noncovid_features_df_shap.iloc[:,:-1].to_numpy().reshape((1738,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "ct_noncovid_features_df_shap['output_encode'] = label_encoder.fit_transform(ct_noncovid_features_df_shap['class_label'])\n",
    "ct_noncovid_features_df_shap\n",
    "ct_noncovid_features_df_shap = pd.get_dummies(ct_noncovid_features_df_shap, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y_shap = np.array(ct_noncovid_features_df_shap[['class_label']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "input_data_x_negative_only_shap = input_data_x_shap\n",
    "output_label_y_negative_only_shap = output_label_y_shap\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x_negative_only_shap.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y_negative_only_shap.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f169968-0003-4dec-96ca-04aef52ef8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Images of Negative Covid-19\n",
    "for i in range(4):\n",
    "    imshow(input_data_x_negative_only_shap[i])\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f96b575-e85e-4ac2-8d64-99b934d86624",
   "metadata": {},
   "source": [
    "#### Prepare positive covid shap-images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e7202-fb66-4ccd-bcae-262c4a260a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x__shap = ct_covid_features_df_shap.iloc[:,:-1].to_numpy().reshape((1980,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "ct_covid_features_df_shap['output_encode'] = label_encoder.fit_transform(ct_covid_features_df_shap['class_label'])\n",
    "ct_covid_features_df_shap\n",
    "ct_covid_features_df_shap = pd.get_dummies(ct_covid_features_df_shap, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y__shap = np.array(ct_covid_features_df_shap[['class_label']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "input_data_x_positive_only_shap = input_data_x__shap\n",
    "output_label_y_positive_only_shap = output_label_y__shap\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x_positive_only_shap.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y_positive_only_shap.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982ac4b-2226-4a05-8fb0-1750744e1081",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Images of Negative Covid-19\n",
    "for i in range(4):\n",
    "    imshow(input_data_x_positive_only_shap[i])\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61627bba-aa64-4e87-a4d2-976474b0d446",
   "metadata": {},
   "source": [
    "#### Prepare both positive and negetive covid shap images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0168f1-b3c6-41d3-afc3-a8823850e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x_shap = cvd_imgs_dataset_shap.iloc[:,:-1].to_numpy().reshape((3718,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_shap['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_shap['class_label'])\n",
    "cvd_imgs_dataset_shap\n",
    "cvd_imgs_dataset_shap = pd.get_dummies(cvd_imgs_dataset_shap, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y_shap = np.array(cvd_imgs_dataset_shap[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x_shap.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y_shap.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b452f-e552-40ee-bdfe-893f5bdd2d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_dataset_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c34d31-c8e0-479f-af16-f4722620d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    input_data_x, output_label_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7896f533-3853-439a-ade0-5b4e89b9dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_shap, test_features_shap, train_labels_shap, test_labels_shap = train_test_split(\n",
    "    input_data_x_shap, output_label_y_shap, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdacb99e-831a-4a74-bd0e-20cb3c4913b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_negative_only, test_features_negative_only, train_labels_negative_only, test_labels_negative_only = train_test_split(\n",
    "    input_data_x_negative_only, output_label_y_negative_only, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a4d179-0a4c-4d45-9499-e0632d1fa665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_positive_only, test_features_positive_only, train_labels_positive_only, test_labels__positive_only = train_test_split(\n",
    "    input_data_x_positive_only, output_label_y_positive_only, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004427c4-5249-47f2-9798-99a7b51ce4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_negative_only_shap, test_features_negative_only_shap, train_labels_negative_only_shap, test_labels_negative_only_shap = train_test_split(\n",
    "    input_data_x_negative_only_shap, output_label_y_negative_only_shap, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6483b01f-c8da-406a-b71a-71e4bd7271f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_positive_only_shap, test_features_positive_only_shap, train_labels_positive_only_shap, test_labels__positive_only_shap = train_test_split(\n",
    "    input_data_x_positive_only_shap, output_label_y_positive_only_shap, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb3e2f0-c872-4c6c-a696-344eaf751a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_shap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8cdef3-365f-4081-b848-1649400c2f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe390163-fa8f-4705-9401-7a4cf140326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_features_positive_only[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e78193-fa63-4ec8-9198-bbaf59a9f4d2",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ba69bb-db28-48f2-9e41-fc5cebb0beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, features, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (array-like): Input image features (e.g., images as numpy arrays).\n",
    "            labels (array-like): Corresponding labels for the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Ensure the image has the correct shape\n",
    "        if image.ndim == 2:  # Grayscale image (height, width)\n",
    "            image = np.expand_dims(image, axis=-1)  # Convert to (height, width, 1)\n",
    "        elif image.ndim == 3 and image.shape[2] == 1:  # If the image is (height, width, 1)\n",
    "            image = np.squeeze(image, axis=-1)  # Convert to (height, width)\n",
    "\n",
    "        # Convert to PIL image\n",
    "        if isinstance(image, np.ndarray):\n",
    "            if image.ndim == 2:  # Grayscale\n",
    "                image = Image.fromarray(image)  # Convert from numpy array to PIL Image\n",
    "            elif image.ndim == 3:  # RGB\n",
    "                image = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82412520-e794-4585-b0ba-d9a481fbb5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Custom normalization for grayscale images (mean and std for single channel)\n",
    "# normalize_grayscale = transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     normalize_grayscale,  # Apply normalization only for grayscale images\n",
    "# ])\n",
    "\n",
    "# Use a conditional normalization based on the number of channels\n",
    "def dynamic_normalize(tensor):\n",
    "    \"\"\"\n",
    "    Normalize tensors dynamically based on the number of channels.\n",
    "    Args:\n",
    "        tensor: A PyTorch tensor representing the image.\n",
    "    Returns:\n",
    "        Normalized tensor.\n",
    "    \"\"\"\n",
    "    if tensor.shape[0] == 3:  # RGB Image\n",
    "        return transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(tensor)\n",
    "    elif tensor.shape[0] == 1:  # Grayscale Image\n",
    "        return transforms.Normalize(mean=[0.485], std=[0.229])(tensor)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported number of channels: {tensor.shape[0]}\")\n",
    "\n",
    "# Add this dynamic normalization into your transform chain:\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    dynamic_normalize,  # Apply normalization dynamically based on channels\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9418fcf5-f8ea-4538-8324-9f895f45ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = CustomImageDataset(features=train_features, labels=train_labels, transform=transform)\n",
    "# #test_dataset = CustomImageDataset(features=test_features, labels=test_labels, transform=transform)\n",
    "\n",
    "# # Create DataLoader instances\n",
    "# loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# #test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0faec4f-84f6-43f5-9f50-d2ff2c5e4832",
   "metadata": {},
   "source": [
    "### Load datset for NonCOVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c74894-d8d9-468c-a8d0-2264ab66f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_negative = CustomImageDataset(features=train_features_negative_only, labels=train_labels_negative_only, transform=transform)\n",
    "#test_dataset = CustomImageDataset(features=test_features, labels=test_labels, transform=transform)\n",
    "\n",
    "# Create DataLoader instances\n",
    "loader_negative = DataLoader(train_dataset_negative, batch_size=64, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd220a6-a04f-4f19-8337-a9cf6ffab399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eceddb-4687-42db-947f-61e173c6b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_negative_shap = CustomImageDataset(features=train_features_negative_only_shap, labels=train_labels_negative_only_shap, transform=transform)\n",
    "#test_dataset = CustomImageDataset(features=test_features, labels=test_labels, transform=transform)\n",
    "\n",
    "# Create DataLoader instances\n",
    "loader_negative_shap = DataLoader(train_dataset_negative_shap, batch_size=64, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa8e980-38ae-4b89-b8f1-dd4c1938ea1f",
   "metadata": {},
   "source": [
    "### Load datset for COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00330b6-11f8-49e6-89af-fa7e505d31df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_positive = CustomImageDataset(features=train_features_positive_only, labels=train_labels_positive_only, transform=transform)\n",
    "#test_dataset = CustomImageDataset(features=test_features, labels=test_labels, transform=transform)\n",
    "\n",
    "# Create DataLoader instances\n",
    "loader_positive = DataLoader(train_dataset_positive, batch_size=64, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e55f9-5583-4b75-8800-ca30ec30c863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b816ba-2fd7-4992-8dce-bee0a6b12c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_positive_shap = CustomImageDataset(features=train_features_positive_only_shap, labels=train_labels_positive_only_shap, transform=transform)\n",
    "#test_dataset = CustomImageDataset(features=test_features, labels=test_labels, transform=transform)\n",
    "\n",
    "# Create DataLoader instances\n",
    "loader_positive_shap = DataLoader(train_dataset_positive_shap, batch_size=64, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af8711-4331-4402-83ca-8703b4f0c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_positive_shap                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f011e9-ad43-4c1a-a583-9f1b4e60b1b2",
   "metadata": {},
   "source": [
    "### Building the multimodal GAN-Transfomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847d138-00b7-4a80-a558-0d8bb3763c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128 * 8 * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Unflatten(1, (128, 8, 8)),\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose2d(128, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # Example Convolutional Layers\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # Additional layers\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(64 * 64 * 64, 1)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"This is: \")\n",
    "        print(x.shape)\n",
    "        x = self.model(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "class GAN(nn.Module):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(GAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "    \n",
    "    def forward(self, x):\n",
    "        generated_image = self.generator(x)\n",
    "        gan_output = self.discriminator(generated_image)\n",
    "        return gan_output\n",
    "\n",
    "# Vision Transformer Model\n",
    "def VisionTransformer(input_shape=(300, 300, 1)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, kernel_size=3, strides=2, padding='same')(inputs)  \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(2 * 2 * 32, activation='relu')(x)\n",
    "    \n",
    "    # Reshaping to scalar output (remove the Conv2DTranspose layers)\n",
    "    x = layers.Dense(1, activation='linear')(x)  # Output a scalar for each image\n",
    "\n",
    "    model = Model(inputs, x)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Save generated images\n",
    "def save_generated_images(epoch, generator, latent_dim, save_dir='gan_generated_images'):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    noise = np.random.randn(100, latent_dim)\n",
    "    generated_images = generator.predict(noise)\n",
    "    generated_images = (generated_images + 1) / 2.0\n",
    "    for i in range(10):\n",
    "        tf.keras.preprocessing.image.save_img(os.path.join(save_dir, f'gen_img_{epoch}_{i}.png'), generated_images[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a4ef4d-6d13-402d-9a5c-02c821a2f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_dim = 100\n",
    "# generator = Generator(latent_dim)\n",
    "# discriminator = Discriminator()\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# generator.to(device)\n",
    "# discriminator.to(device)\n",
    "# gan = GAN(generator, discriminator)\n",
    "\n",
    "# # Vision Transformer training setup\n",
    "# vit_model = VisionTransformer(input_shape=(300, 300, 1))\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# # Train GAN\n",
    "# def train_gan(generator, discriminator, gan, dataloader, latent_dim, n_epochs, n_batch):\n",
    "#     # Define optimizer\n",
    "#     discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "#     generator_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "#     criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy loss\n",
    "\n",
    "#     for epoch in range(n_epochs):\n",
    "#         for batch in dataloader:\n",
    "#             # Get real images\n",
    "            \n",
    "#             real_images = batch[0].to(device)\n",
    "#             # Generate fake images\n",
    "#             noise = torch.randn(n_batch, latent_dim).to(device)\n",
    "#             fake_images = generator(noise)\n",
    "\n",
    "#             # Labels for real and fake images\n",
    "#             real_labels = torch.ones(n_batch, 1).to(device)\n",
    "#             fake_labels = torch.zeros(n_batch, 1).to(device)\n",
    "#             fake_images_resized = F.interpolate(fake_images, size=(300, 300), mode='bilinear', align_corners=False)\n",
    "            \n",
    "            \n",
    "#             # Train the discriminator\n",
    "#             discriminator_optimizer.zero_grad()\n",
    "\n",
    "#             # Concatenate real and fake images\n",
    "#             d_input = torch.cat((real_images, fake_images_resized))\n",
    "#             d_labels = torch.cat((real_labels, fake_labels))\n",
    "\n",
    "#             # Get the discriminator output (shape: [n_batch * 2, 1])\n",
    "#             d_output = discriminator(d_input)\n",
    "            \n",
    "#             # Compute the discriminator loss\n",
    "#             d_labels_ = d_labels[:d_output.size(0)]\n",
    "#             d_loss = criterion(d_output, d_labels_)\n",
    "#             d_loss.backward()\n",
    "\n",
    "#             # Update discriminator\n",
    "#             discriminator_optimizer.step()\n",
    "\n",
    "#             # Train the generator\n",
    "#             generator_optimizer.zero_grad()\n",
    "\n",
    "#             # Generate new noise for the generator\n",
    "#             noise = torch.randn(n_batch, latent_dim).to(device)\n",
    "\n",
    "#             # Get the generator's output for the fake images\n",
    "#             g_output = generator(noise)\n",
    "#             g_output_resized = F.interpolate(g_output, size=(300, 300), mode='bilinear', align_corners=False)\n",
    "\n",
    "#             # Labels for the generator are all real (1) since we want the generator to fool the discriminator\n",
    "#             g_labels = torch.ones(n_batch, 1).to(device)\n",
    "\n",
    "#             # Get the discriminator's output for the generator's fake images\n",
    "#             g_d_output = discriminator(g_output_resized)\n",
    "\n",
    "#             # Compute the generator loss\n",
    "#             g_loss = criterion(g_d_output, g_labels)\n",
    "#             g_loss.backward()\n",
    "\n",
    "#             # Update generator\n",
    "#             generator_optimizer.step()\n",
    "\n",
    "#         print(f\"Epoch {epoch+1}/{n_epochs} | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.image import smart_resize\n",
    "\n",
    "# def preprocess_images(images):\n",
    "#     images_resized = tf.image.resize(images, (30, 30))\n",
    "#     return images_resized\n",
    "\n",
    "\n",
    "# # Train Vision Transformer\n",
    "# def train_vit(vit_model, dataloader, n_epochs=10, n_batch=32):\n",
    "#     for epoch in range(n_epochs):\n",
    "#         for batch in dataloader:\n",
    "#             images, labels = batch\n",
    "#             # Ensure images and labels have matching batch sizes\n",
    "#             if images.size(0) == labels.size(0):\n",
    "#                 images = tf.convert_to_tensor(images.cpu().numpy().reshape(-1, 300, 300, 1))\n",
    "#                 labels = tf.convert_to_tensor(labels.cpu().numpy())\n",
    "#                 vit_model.fit(images, labels)\n",
    "#             else:\n",
    "#                 print(f\"Batch size mismatch: {images.size(0)} images, {labels.size(0)} labels\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Training process\n",
    "# train_gan(generator, discriminator, gan, loader_positive, latent_dim, n_epochs=10, n_batch=64)\n",
    "# train_vit(vit_model, loader_positive_shap, n_epochs=10, n_batch=192)\n",
    "\n",
    "# # Save ViT output as images\n",
    "# def save_vit_output(vit_model, dataset, save_dir='/home/maxwellsam/Multimodal-COVID19-Images/generatedImages/Transformer/COVID-19/'):\n",
    "#     if not os.path.exists(save_dir):\n",
    "#         os.makedirs(save_dir)\n",
    "#     predictions = vit_model.predict(dataset)\n",
    "#     for i in range(10):\n",
    "#         img = tf.image.resize(predictions[i], (30, 30))\n",
    "#         tf.keras.preprocessing.image.save_img(os.path.join(save_dir, f'vit_img_{i}.png'), img)\n",
    "\n",
    "# save_vit_output(vit_model, train_dataset_positive_shap[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b948dd4b-620b-4aea-82b7-bee420072e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator_(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator_, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 64, kernel_size=4, stride=2, padding=1),  # Conv layer 1\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # Conv layer 2\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # Conv layer 3\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),  # Conv layer 4\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten(),  # Flatten the tensor to pass it to the fully connected layers\n",
    "            nn.Linear(512 * 8 * 8, 1),  # Fully connected layer\n",
    "            nn.Sigmoid()  # Output a probability\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93529e66-bc11-4890-9cdc-d39c2fcc8f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8b8dc-ee00-4619-a496-92eb8b38923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "generator = Generator(latent_dim)\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebebd63-7361-4676-ac5d-688f716474c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tensor = torch.randn(64, 3, 64, 64)  \n",
    "# output = discriminator(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbcac1c-5ab8-4aa7-8e68-d112aab12890",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(generator, discriminator)\n",
    "vit_model = VisionTransformer(input_shape=(300, 300, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea6d1f-fdf7-4280-9c5b-bf51a8eb05fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "\n",
    "def train_multimodal_gan(generator, discriminator, gan, vit_model, loader_gan, loader_vit, latent_dim, num_epochs=10, n_batch=64, save_dir='generated_images/', device='cuda'):\n",
    "    \"\"\"\n",
    "    Trains the GAN and ViT models on separate datasets, saves generated images to a directory,\n",
    "    and logs to TensorBoard.\n",
    "    \n",
    "    Parameters:\n",
    "    - generator: GAN generator model.\n",
    "    - discriminator: GAN discriminator model.\n",
    "    - gan: The complete GAN model.\n",
    "    - vit_model: Pre-trained Vision Transformer model.\n",
    "    - loader_gan: Dataset for GAN training.\n",
    "    - loader_vit: Dataset for ViT training.\n",
    "    - latent_dim: Latent vector dimension for GAN.\n",
    "    - num_epochs: Number of epochs to train the model.\n",
    "    - n_batch: Batch size.\n",
    "    - save_dir: Directory to save the images.\n",
    "    - device: Device to use for training (e.g., 'cuda' or 'cpu').\n",
    "    \"\"\"\n",
    "    # TensorBoard writers\n",
    "    writer_fake = SummaryWriter('runs/GANViT_MNIST/fake')\n",
    "    writer_real = SummaryWriter('runs/GANViT_MNIST/real')\n",
    "    writer_losses = SummaryWriter('runs/GANViT_MNIST/losses')\n",
    "\n",
    "    # Start TensorBoard\n",
    "    %tensorboard --logdir=runs/CNN_GAN_MNIST/\n",
    "\n",
    "    # Create the directory if it does not exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    step = 0\n",
    "    fixed_noise = torch.randn(n_batch, latent_dim, device=device) \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (real, _) in enumerate(loader_gan):  \n",
    "            real = real.to(device)\n",
    "            batch_size = real.shape[0]\n",
    "\n",
    "            # Train Discriminator\n",
    "            noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake = generator(noise)\n",
    "            print(fake.shape)\n",
    "            \n",
    "            disc_real = discriminator(real).view(-1)\n",
    "            lossD_real = torch.nn.functional.binary_cross_entropy(disc_real, torch.ones_like(disc_real))\n",
    "            \n",
    "            disc_fake = discriminator(fake.detach()).view(-1)\n",
    "            lossD_fake = torch.nn.functional.binary_cross_entropy(disc_fake, torch.zeros_like(disc_fake))\n",
    "            lossD = (lossD_real + lossD_fake) / 2\n",
    "            discriminator.zero_grad()\n",
    "            lossD.backward()\n",
    "            discriminator.optim.step()\n",
    "\n",
    "            # Train Generator\n",
    "            output = discriminator(fake).view(-1)\n",
    "            lossG = torch.nn.functional.binary_cross_entropy(output, torch.ones_like(output))\n",
    "            generator.zero_grad()\n",
    "            lossG.backward()\n",
    "            generator.optim.step()\n",
    "\n",
    "            # Print loss every 100 batches\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader_gan)} \"\n",
    "                      f\"Loss D: {lossD:.4f}, Loss G: {lossG:.4f}\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    fake_images = generator(fixed_noise)\n",
    "                    img_grid_fake = torchvision.utils.make_grid(fake_images, normalize=True)\n",
    "                    img_grid_real = torchvision.utils.make_grid(real, normalize=True)\n",
    "\n",
    "                    # Log images to TensorBoard\n",
    "                    writer_fake.add_image(\"Fake Images\", img_grid_fake, global_step=step)\n",
    "                    writer_real.add_image(\"Real Images\", img_grid_real, global_step=step)\n",
    "                    writer_losses.add_scalar(\"Discriminator Loss\", lossD, global_step=step)\n",
    "                    writer_losses.add_scalar(\"Generator Loss\", lossG, global_step=step)\n",
    "\n",
    "                    # Save generated images to disk\n",
    "                    save_generated_images(fake_images, real, epoch, batch_idx, save_dir)\n",
    "\n",
    "                step += 1\n",
    "\n",
    "        # After each epoch, save generated images to a directory\n",
    "        save_generated_images(fake_images, real, epoch, batch_idx, save_dir)\n",
    "\n",
    "    writer_fake.close()\n",
    "    writer_real.close()\n",
    "    writer_losses.close()\n",
    "\n",
    "\n",
    "def save_generated_images(fake_images, real_images, epoch, batch_idx, save_dir):\n",
    "    \"\"\"\n",
    "    Saves the generated images after each batch or epoch to the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    - fake_images: Generated images by the GAN model.\n",
    "    - real_images: Real images from the dataset.\n",
    "    - epoch: Current epoch number.\n",
    "    - batch_idx: Current batch index.\n",
    "    - save_dir: Directory to save the images.\n",
    "    \"\"\"\n",
    "    # Save the real and fake images to disk\n",
    "    for i, img in enumerate(fake_images):\n",
    "        img_path = os.path.join(save_dir, f'fake_epoch_{epoch+1}_batch_{batch_idx+1}_img_{i+1}.png')\n",
    "        torchvision.utils.save_image(img, img_path)\n",
    "\n",
    "    for i, img in enumerate(real_images):\n",
    "        img_path = os.path.join(save_dir, f'real_epoch_{epoch+1}_batch_{batch_idx+1}_img_{i+1}.png')\n",
    "        torchvision.utils.save_image(img, img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a4f02-f143-424a-8e16-593e04a24d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_multimodal_gan(generator, discriminator, gan, vit_model, loader_positive, loader_positive_shap, latent_dim=100, num_epochs=10, n_batch=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148230eb-7b77-43ac-abdc-03f87a1c3b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20de922-fb08-4a00-b82e-0a8304bbfca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
