{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea1b12-93d5-4ad0-90c7-7fd0460ea161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd88084-324f-4378-bebf-e54de39aa5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os.path\n",
    "from PIL import Image\n",
    "from skimage.io import imread, imsave, imshow, show, imread_collection, imshow_collection\n",
    "import os, logging\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'  \n",
    "from os import listdir\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "#import keras_tuner as kt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tensorflow import keras\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "os.environ[\"TF_CPP_VMODULE\"]=\"gpu_process_state=10,gpu_cudamallocasync_allocator=10\"\n",
    "a = tf.zeros([], tf.float32)\n",
    "## Imports libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078478b-7170-4af0-b764-f5e2480c692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07cc7c-34de-4259-a6cb-b0d560369db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renameImageFiles(folderpath, prefix,fileExtension):\n",
    "    folder_path = folderpath\n",
    "    new_prefix = prefix\n",
    "\n",
    "    for i, file_path in enumerate(glob.glob(folder_path + '*.'+fileExtension)):\n",
    "        new_file_name = new_prefix + '_' + str(i+1) + '.'+fileExtension\n",
    "        os.rename(file_path, os.path.join(folder_path, new_file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc314d8-3523-4c5c-83c0-e0c5ef3b3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/home/maxwellsam/Multimodal-COVID19-Images/dataset/CT_COVID/'\n",
    "prefix1 = 'ct_covid'\n",
    "path2 = '/home/maxwellsam/Multimodal-COVID19-Images/dataset/CT_NonCOVID/'\n",
    "prefix2 = 'ct_noncovid'\n",
    "\n",
    "# renameImageFiles(path1, prefix1,'png')\n",
    "# renameImageFiles(path2, prefix2,'png')\n",
    "# renameImageFiles(path2, prefix2,'jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a99cd28-c5b5-422b-a3c4-79daeecc2ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImages(imgDirPath, binary_label):\n",
    "    img_names = list()\n",
    "    try:\n",
    "        with os.scandir(imgDirPath) as dirs:\n",
    "            for entry in dirs:\n",
    "                img_names.append(entry.name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scanning directory: {e}\")\n",
    "        return None\n",
    "\n",
    "    all_features = []\n",
    "    for img in img_names:\n",
    "        try:\n",
    "            path = imgDirPath + img\n",
    "            cv_img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if cv_img is None:\n",
    "                print(f\"Error reading image: {path}\")\n",
    "                continue\n",
    "\n",
    "            cv_img2 = cv2.resize(cv_img, (300, 300), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            nFeatures = (cv_img2.shape[0] * cv_img2.shape[1])\n",
    "            features = np.reshape(cv_img2, nFeatures)\n",
    "            all_features.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img}: {e}\")\n",
    "\n",
    "    if len(all_features) == 0:\n",
    "        print(\"No valid images found.\")\n",
    "        return None\n",
    "\n",
    "    imgs_df = pd.DataFrame(np.array(all_features), index=img_names)\n",
    "    if binary_label == 0:\n",
    "        imgs_df['class_label'] = np.zeros((imgs_df.shape[0]), dtype=int)\n",
    "    else:\n",
    "        imgs_df['class_label'] = np.ones((imgs_df.shape[0]), dtype=int)\n",
    "\n",
    "    return imgs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bc82c6-c44d-47f9-bde6-ff4027a2a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df =  processImages(path1,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df =  processImages(path2,0)#0 ---> covnid-19 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ed399-eb84-4984-8087-52031fa1664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs = [ct_noncovid_features_df, ct_covid_features_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset = cvd_imgs_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c53e333-dc48-47dc-8eaf-88e0d6eb4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_noncovid_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5212b76c-4ba0-47b4-aa9e-06ea0846c9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5042e05-3ad4-4c52-9e6d-2beba1529a9d",
   "metadata": {},
   "source": [
    "#### Prepare negative covid images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0553ca-9648-46ba-a516-cc6450e2880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x_ = ct_noncovid_features_df.iloc[:,:-1].to_numpy().reshape((2173,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "ct_noncovid_features_df['output_encode'] = label_encoder.fit_transform(ct_noncovid_features_df['class_label'])\n",
    "ct_noncovid_features_df\n",
    "ct_noncovid_features_df = pd.get_dummies(ct_noncovid_features_df, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y_ = np.array(ct_noncovid_features_df[['class_label']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "input_data_x_negative_only = input_data_x_\n",
    "output_label_y_negative_only = output_label_y_\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x_negative_only.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y_negative_only.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea13fb-9d18-49ca-aa97-fbf08aed4a79",
   "metadata": {},
   "source": [
    "#### Prepare positive covid images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d2452-b1a2-4f72-8828-c257238acbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x__ = ct_covid_features_df.iloc[:,:-1].to_numpy().reshape((2476,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "ct_covid_features_df['output_encode'] = label_encoder.fit_transform(ct_covid_features_df['class_label'])\n",
    "ct_covid_features_df\n",
    "ct_covid_features_df = pd.get_dummies(ct_covid_features_df, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y__ = np.array(ct_covid_features_df[['class_label']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "input_data_x_positive_only = input_data_x__\n",
    "output_label_y_positive_only = output_label_y__\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x_positive_only.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y_positive_only.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6d361-36f3-456b-b9f3-8abd22ee24f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_negative_only, test_features_negative_only, train_labels_negative_only, test_labels_negative_only = train_test_split(\n",
    "    input_data_x_negative_only, output_label_y_negative_only, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f791f28-f645-4245-ad1d-af6af811cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_positive_only, test_features_positive_only, train_labels_positive_only, test_labels__positive_only = train_test_split(\n",
    "    input_data_x_positive_only, output_label_y_positive_only, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c85be-730c-4440-a870-eb5db868cfc3",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa597f6-36fd-439d-8916-6b48ca5735f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, features, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (array-like): Input image features (e.g., images as numpy arrays).\n",
    "            labels (array-like): Corresponding labels for the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Ensure the image has the correct shape\n",
    "        if image.ndim == 2:  # Grayscale image (height, width)\n",
    "            image = np.expand_dims(image, axis=-1)  # Convert to (height, width, 1)\n",
    "        elif image.ndim == 3 and image.shape[2] == 1:  # If the image is (height, width, 1)\n",
    "            image = np.squeeze(image, axis=-1)  # Convert to (height, width)\n",
    "\n",
    "        # Convert to PIL image\n",
    "        if isinstance(image, np.ndarray):\n",
    "            if image.ndim == 2:  # Grayscale\n",
    "                image = Image.fromarray(image)  # Convert from numpy array to PIL Image\n",
    "            elif image.ndim == 3:  # RGB\n",
    "                image = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32264260-6086-4288-a486-ead91f9f8fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Custom normalization for grayscale images (mean and std for single channel)\n",
    "# normalize_grayscale = transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     normalize_grayscale,  # Apply normalization only for grayscale images\n",
    "# ])\n",
    "\n",
    "# Use a conditional normalization based on the number of channels\n",
    "def dynamic_normalize(tensor):\n",
    "    \"\"\"\n",
    "    Normalize tensors dynamically based on the number of channels.\n",
    "    Args:\n",
    "        tensor: A PyTorch tensor representing the image.\n",
    "    Returns:\n",
    "        Normalized tensor.\n",
    "    \"\"\"\n",
    "    if tensor.shape[0] == 3:  # RGB Image\n",
    "        return transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(tensor)\n",
    "    elif tensor.shape[0] == 1:  # Grayscale Image\n",
    "        return transforms.Normalize(mean=[0.485], std=[0.229])(tensor)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported number of channels: {tensor.shape[0]}\")\n",
    "\n",
    "# Add this dynamic normalization into your transform chain:\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    dynamic_normalize,  # Apply normalization dynamically based on channels\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4f0bcc-ae89-4335-80a2-215b3a0201bb",
   "metadata": {},
   "source": [
    "### Load datset for NonCOVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ad5b0-f43b-41bf-80c0-707040d849cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_negative = CustomImageDataset(features=train_features_negative_only, labels=train_labels_negative_only, transform=transform)\n",
    "#test_dataset = CustomImageDataset(features=test_features, labels=test_labels, transform=transform)\n",
    "\n",
    "# Create DataLoader instances\n",
    "loader_negative = DataLoader(train_dataset_negative, batch_size=64, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea20e8-7774-4c20-99de-8bcdb4d3d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(loader_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76880e08-2176-440e-b45e-7e0e612ede7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader_negative:\n",
    "    images, labels = batch \n",
    "    \n",
    "    # Convert images from PyTorch tensor to NumPy array\n",
    "    images_np = images.numpy()\n",
    "    \n",
    "    # Convert the NumPy array to TensorFlow tensor\n",
    "    images_tf = tf.convert_to_tensor(images_np, dtype=tf.float32)\n",
    "    \n",
    "print(images_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed4ab71-865f-4a7d-85a4-c500d0b86622",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_images = images_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91924d6-4237-4e19-9d68-63b22e924a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb4d322-a174-4fde-8159-f0e0d40179bb",
   "metadata": {},
   "source": [
    "### Load datset for COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e635f-422d-4fff-bb48-c55f924ad7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_positive = CustomImageDataset(features=train_features_positive_only, labels=train_labels_positive_only, transform=transform)\n",
    "#test_dataset = CustomImageDataset(features=test_features, labels=test_labels, transform=transform)\n",
    "\n",
    "# Create DataLoader instances\n",
    "loader_positive = DataLoader(train_dataset_positive, batch_size=64, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b719b510-a986-4b5f-91e6-94132c150989",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader_positive:\n",
    "    images_, labels_ = batch \n",
    "    \n",
    "    # Convert images from PyTorch tensor to NumPy array\n",
    "    images_np_ = images.numpy()\n",
    "    #print(images_np_.shape)\n",
    "    # Convert the NumPy array to TensorFlow tensor\n",
    "    images_tf_ = tf.convert_to_tensor(images_np, dtype=tf.float32)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26e0f3e-a257-4029-bcdc-04d32fe039cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_images = images_tf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca4249a-8050-42c9-9a95-80a4fd619ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(positive_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6cc12-e283-4804-aaeb-8841e8340002",
   "metadata": {},
   "source": [
    "#### GAN-VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f1796-69bd-452f-a86f-26d0859ea653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define input image dimensions\n",
    "img_height, img_width, channels = 300, 300, 1\n",
    "latent_dim = 100  # Dimension of latent space\n",
    "\n",
    "# Directory for saving outputs\n",
    "output_dir = \"/home/maxwellsam/Multimodal-COVID19-Images/generatedImages/GAN-VAE/Non-COVID-19/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the encoder\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(32, (3, 3), activation=\"relu\", strides=2, padding=\"same\")\n",
    "        self.conv2 = layers.Conv2D(64, (3, 3), activation=\"relu\", strides=2, padding=\"same\")\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(128, activation=\"relu\")\n",
    "        self.mu = layers.Dense(10)  # Mean vector\n",
    "        self.logvar = layers.Dense(10)  # Log variance vector\n",
    "\n",
    "    def call(self, input_images):\n",
    "        x = self.conv1(input_images)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        mu = self.mu(x)\n",
    "        logvar = self.logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "# Define the decoder\n",
    "class Decoder(Model):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = layers.Dense(16 * 16 * 64, activation='relu')\n",
    "        self.reshape = layers.Reshape((16, 16, 64))\n",
    "        self.deconv1 = layers.Conv2DTranspose(64, (3, 3), activation='relu', strides=2, padding='same')\n",
    "        self.deconv2 = layers.Conv2DTranspose(32, (3, 3), activation='relu', strides=2, padding='same')\n",
    "        self.out_layer = layers.Conv2DTranspose(channels, (3, 3), activation='sigmoid', padding='same')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.reshape(x)\n",
    "        x = self.deconv1(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.out_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the discriminator for GAN\n",
    "class Discriminator(Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(64, (3, 3), strides=2, padding='same')\n",
    "        self.conv2 = layers.Conv2D(128, (3, 3), strides=2, padding='same')\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc = layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Reparameterization trick\n",
    "@tf.function\n",
    "def reparameterize(mu, logvar):\n",
    "    eps = tf.random.normal(shape=mu.shape)\n",
    "    return eps * tf.exp(logvar * 0.5) + mu\n",
    "\n",
    "# Loss functions\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "def vae_loss(x, x_recon, mu, logvar):\n",
    "    x_recon = tf.image.resize(x_recon, (300, 300))\n",
    "    reconstruction_loss = tf.reduce_mean(tf.keras.losses.mean_squared_error(x, x_recon))\n",
    "    kl_divergence = -0.5 * tf.reduce_sum(1 + logvar - tf.square(mu) - tf.exp(logvar))\n",
    "    return reconstruction_loss + kl_divergence\n",
    "\n",
    "# Models\n",
    "encoder =    Encoder()\n",
    "decoder = Decoder()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Optimizers\n",
    "vae_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "gan_optimizer = tf.keras.optimizers.Adam(1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dc61a2-38c8-4635-a0b5-abc13bd474a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56211bd6-9184-49a3-8e04-526ba9fb3f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard setup\n",
    "log_dir = \"logs/gan_vae\"\n",
    "tensorboard_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# Training step\n",
    "\n",
    "def train_step(images):\n",
    "    # Ensure models are initialized\n",
    "    # _ = encoder(tf.random.normal([1, 300, 300, 1]))\n",
    "    # _ = decoder(tf.random.normal([1, latent_dim]))\n",
    "    # _ = discriminator(tf.random.normal([1, 300, 300, 1]))\n",
    "\n",
    "\n",
    "    vae_optimizer.build(encoder.trainable_variables + decoder.trainable_variables)\n",
    "    gan_optimizer.build(discriminator.trainable_variables + decoder.trainable_variables)\n",
    "\n",
    "    # VAE forward pass\n",
    "    with tf.GradientTape() as vae_tape:\n",
    "        mu, logvar = encoder(images)\n",
    "        z = reparameterize(mu, logvar)\n",
    "        reconstructed = decoder(z)\n",
    "        loss_vae = vae_loss(images, reconstructed, mu, logvar)\n",
    "\n",
    "    vae_gradients = vae_tape.gradient(loss_vae, encoder.trainable_variables + decoder.trainable_variables)\n",
    "    vae_optimizer.apply_gradients(zip(vae_gradients, encoder.trainable_variables + decoder.trainable_variables))\n",
    "\n",
    "    # GAN forward pass\n",
    "    with tf.GradientTape() as disc_tape, tf.GradientTape() as gen_tape:\n",
    "        generated_images = decoder(tf.random.normal([images.shape[0], latent_dim]))\n",
    "        resized_generated_images = tf.image.resize(generated_images, (300, 300))\n",
    "        resized_generated_images = tf.clip_by_value(resized_generated_images, 0.0, 1.0)  # Ensure valid pixel values\n",
    "\n",
    "        real_output = discriminator(images)\n",
    "        fake_output = discriminator(resized_generated_images)\n",
    "\n",
    "        loss_discriminator = (\n",
    "            bce_loss(tf.ones_like(real_output), real_output) + \n",
    "            bce_loss(tf.zeros_like(fake_output), fake_output)\n",
    "        )\n",
    "        loss_generator = bce_loss(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "    discriminator_gradients = disc_tape.gradient(loss_discriminator, discriminator.trainable_variables)\n",
    "    generator_gradients = gen_tape.gradient(loss_generator, decoder.trainable_variables)\n",
    "\n",
    "\n",
    "    gan_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
    "    gan_optimizer.apply_gradients(zip(generator_gradients, decoder.trainable_variables))\n",
    "\n",
    "    return loss_vae, loss_discriminator, loss_generator\n",
    "\n",
    "#%tensorboard --logdir=runs/GAN_VAE_MNIST/\n",
    "\n",
    "# Training loop\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, batch in dataset:\n",
    "            # Preprocess the PyTorch batch\n",
    "            #print(f\"Input shape to the discriminator: {batch.shape}\")\n",
    "            images = preprocess_images_(batch)\n",
    "\n",
    "            # Run the training step\n",
    "            loss_vae, loss_discriminator, loss_generator = train_step(images)\n",
    "\n",
    "            # Log to TensorBoard\n",
    "            with tensorboard_writer.as_default():\n",
    "                step = epoch * len(dataset) + batch_idx\n",
    "                tf.summary.scalar('VAE Loss', loss_vae, step=step)\n",
    "                tf.summary.scalar('Discriminator Loss', loss_discriminator, step=step)\n",
    "                tf.summary.scalar('Generator Loss', loss_generator, step=step)\n",
    "\n",
    "        # Save generated images at the end of each epoch\n",
    "        generated_images = decoder(tf.random.normal([images.shape[0], latent_dim]))\n",
    "        resized_generated_images = tf.image.resize(generated_images, (300, 300))\n",
    "        save_images(resized_generated_images, epoch)\n",
    "        print(f\"Epoch {epoch + 1}, VAE Loss: {loss_vae:.4f}, Discriminator Loss: {loss_discriminator:.4f}, Generator Loss: {loss_generator:.4f}\")\n",
    "\n",
    "\n",
    "# Save images to output directory\n",
    "def save_images(images, epoch):\n",
    "    images = (images - tf.reduce_min(images)) / (tf.reduce_max(images) - tf.reduce_min(images)) \n",
    "    images = (images * 255).numpy().astype(np.uint8)  \n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        # Save each image (ensure img is 2D for grayscale)\n",
    "        tf.keras.utils.save_img(\n",
    "            f\"{output_dir}/epoch_{epoch+1}_img_{i+1}.png\",\n",
    "            img,\n",
    "            data_format=\"channels_last\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Load dataset \n",
    "def preprocess_images_(image):\n",
    "    image = tf.transpose(image, perm=[1, 2, 0])\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    \n",
    "    return image / 255.0 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18847c74-4462-4c47-b4d0-916d623a7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train(negative_images, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de1728-4ea0-4c19-83b2-b5ca823aae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(negative_images.shape)\n",
    "image_____ = tf.transpose(negative_images, perm=[0, 2, 3, 1])\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bcd3e4-7f98-4a48-b5b4-2e5f1bf13e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.io import imshow, show\n",
    "\n",
    "# Ensure the image is a NumPy array with a proper dtype (e.g., uint8 for image data)\n",
    "for batch in negative_images:\n",
    "    for i in range(1):\n",
    "        imgd = batch[i].numpy()  # Convert tensor to NumPy array\n",
    "        img = np.clip(imgd, 0, 255).astype(np.uint8)  # Ensure the values are within the expected range\n",
    "        imshow(img)\n",
    "        show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f66533-f5f7-4825-904a-f821609bb172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a23308-4c1b-4ebc-befb-c786bc421da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f939d7-1fe4-4260-904d-b5deb8ec9b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
