{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826871b2-b4c2-4f06-948f-1fc15fb62288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3c6071-6207-4aec-ae90-c48feeaebbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005af1d6-17ab-478f-afef-3f894a3f95f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os.path\n",
    "from PIL import Image\n",
    "from skimage.io import imread, imsave, imshow, show, imread_collection, imshow_collection\n",
    "import os, logging\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'  \n",
    "from os import listdir\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "#import keras_tuner as kt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tensorflow import keras\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "os.environ[\"TF_CPP_VMODULE\"]=\"gpu_process_state=10,gpu_cudamallocasync_allocator=10\"\n",
    "a = tf.zeros([], tf.float32)\n",
    "\n",
    "## Imports libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e7aef-3df9-40d4-836d-b126c784be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import clear_output\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd700ed5-e397-4ff9-b5e8-ff346ea89934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renameImageFiles(folderpath, prefix,fileExtension):\n",
    "    folder_path = folderpath\n",
    "    new_prefix = prefix\n",
    "\n",
    "    for i, file_path in enumerate(glob.glob(folder_path + '*.'+fileExtension)):\n",
    "        new_file_name = new_prefix + '_' + str(i+1) + '.'+fileExtension\n",
    "        os.rename(file_path, os.path.join(folder_path, new_file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8139e06-0ac5-4a2f-8751-3b023fa16095",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43d606-28a3-4529-bf31-ba5a48804f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/home/maxwellsam/Multimodal-COVID19-Images/dataset/CT_COVID/'\n",
    "prefix1 = 'ct_covid'\n",
    "path2 = '/home/maxwellsam/Multimodal-COVID19-Images/dataset/CT_NonCOVID/'\n",
    "prefix2 = 'ct_noncovid'\n",
    "\n",
    "# renameImageFiles(path1, prefix1,'png')\n",
    "# renameImageFiles(path2, prefix2,'png')\n",
    "# renameImageFiles(path2, prefix2,'jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c16b27-c2cb-46a6-ab18-88eebdc78c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "path3 = '/home/maxwellsam/Multimodal-COVID19-Images/dataset/Shap/CT_COVID/'\n",
    "prefix3 = 'ct_covid'\n",
    "path4 = '/home/maxwellsam/Multimodal-COVID19-Images/dataset/Shap/CT_NonCOVID/'\n",
    "prefix4 = 'ct_noncovid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b3c26-6676-47f5-92e7-4028af48b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImages(imgDirPath, binary_label):\n",
    "    img_names = list()\n",
    "    try:\n",
    "        with os.scandir(imgDirPath) as dirs:\n",
    "            for entry in dirs:\n",
    "                img_names.append(entry.name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scanning directory: {e}\")\n",
    "        return None\n",
    "\n",
    "    all_features = []\n",
    "    for img in img_names:\n",
    "        try:\n",
    "            path = imgDirPath + img\n",
    "            cv_img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if cv_img is None:\n",
    "                print(f\"Error reading image: {path}\")\n",
    "                continue\n",
    "\n",
    "            cv_img2 = cv2.resize(cv_img, (300, 300), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            nFeatures = (cv_img2.shape[0] * cv_img2.shape[1])\n",
    "            features = np.reshape(cv_img2, nFeatures)\n",
    "            all_features.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img}: {e}\")\n",
    "\n",
    "    if len(all_features) == 0:\n",
    "        print(\"No valid images found.\")\n",
    "        return None\n",
    "\n",
    "    imgs_df = pd.DataFrame(np.array(all_features), index=img_names)\n",
    "    if binary_label == 0:\n",
    "        imgs_df['class_label'] = np.zeros((imgs_df.shape[0]), dtype=int)\n",
    "    else:\n",
    "        imgs_df['class_label'] = np.ones((imgs_df.shape[0]), dtype=int)\n",
    "\n",
    "    return imgs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c57e34-4ffb-44de-b31c-7d17da669ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df =  processImages(path1,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df =  processImages(path2,0)#0 ---> covnid-19 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301cba6-be1a-4098-a1e8-d888515b4eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df_shap =  processImages(path3,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df_shap =  processImages(path4,0)#0 ---> covnid-19 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76129c40-46be-4cdc-9d7d-05b7c271dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs = [ct_noncovid_features_df, ct_covid_features_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset = cvd_imgs_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db2820-97c9-4eaa-8278-b66835afa21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_shap = [ct_noncovid_features_df_shap, ct_covid_features_df_shap]\n",
    "cvd_imgs_dataset_shap = pd.concat(cvd_imgs_shap)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset_shap = cvd_imgs_dataset_shap.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset_shap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da92581f-76e4-4814-9fdb-488a92333af8",
   "metadata": {},
   "source": [
    "#### Prepare negative covid images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c818e-d86d-470e-98b5-54158757cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x_ = ct_noncovid_features_df.iloc[:,:-1].to_numpy().reshape((2173,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "ct_noncovid_features_df['output_encode'] = label_encoder.fit_transform(ct_noncovid_features_df['class_label'])\n",
    "ct_noncovid_features_df\n",
    "ct_noncovid_features_df = pd.get_dummies(ct_noncovid_features_df, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y_ = np.array(ct_noncovid_features_df[['class_label']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "input_data_x_negative_only = input_data_x_\n",
    "output_label_y_negative_only = output_label_y_\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x_negative_only.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y_negative_only.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dd2e23-c04c-4b28-9905-1cf3a2f13632",
   "metadata": {},
   "source": [
    "#### Prepare positive covid images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd730d-d06a-4f4c-93c6-0f0e13dd06f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x__ = ct_covid_features_df.iloc[:,:-1].to_numpy().reshape((2476,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "ct_covid_features_df['output_encode'] = label_encoder.fit_transform(ct_covid_features_df['class_label'])\n",
    "ct_covid_features_df\n",
    "ct_covid_features_df = pd.get_dummies(ct_covid_features_df, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y__ = np.array(ct_covid_features_df[['class_label']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "input_data_x_positive_only = input_data_x__\n",
    "output_label_y_positive_only = output_label_y__\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x_positive_only.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y_positive_only.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40129d3-cfe2-40a7-a92e-7e9d7bcb648d",
   "metadata": {},
   "source": [
    "#### Prepare both positive and negetive covid images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf49cf6-245c-4a59-8057-7b4b085e48a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy().reshape((4649,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset['class_label'])\n",
    "cvd_imgs_dataset\n",
    "cvd_imgs_dataset = pd.get_dummies(cvd_imgs_dataset, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y = np.array(cvd_imgs_dataset[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ee1fee-5c3c-4c83-a2c5-a35c0355ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61627bba-aa64-4e87-a4d2-976474b0d446",
   "metadata": {},
   "source": [
    "#### Prepare both positive and negetive covid shap images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0168f1-b3c6-41d3-afc3-a8823850e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x_shap = cvd_imgs_dataset_shap.iloc[:,:-1].to_numpy().reshape((3718,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "cvd_imgs_dataset_shap['output_encode'] = label_encoder.fit_transform(cvd_imgs_dataset_shap['class_label'])\n",
    "cvd_imgs_dataset_shap\n",
    "cvd_imgs_dataset_shap = pd.get_dummies(cvd_imgs_dataset_shap, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y_shap = np.array(cvd_imgs_dataset_shap[['output_encode_0','output_encode_1']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x_shap.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y_shap.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b452f-e552-40ee-bdfe-893f5bdd2d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs_dataset_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c34d31-c8e0-479f-af16-f4722620d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    input_data_x, output_label_y, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7896f533-3853-439a-ade0-5b4e89b9dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_shap, test_features_shap, train_labels_shap, test_labels_shap = train_test_split(\n",
    "    input_data_x_shap, output_label_y_shap, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdacb99e-831a-4a74-bd0e-20cb3c4913b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_negative_only, test_features_negative_only, train_labels_negative_only, test_labels_negative_only = train_test_split(\n",
    "    input_data_x_negative_only, output_label_y_negative_only, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a4d179-0a4c-4d45-9499-e0632d1fa665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_positive_only, test_features_positive_only, train_labels_positive_only, test_labels__positive_only = train_test_split(\n",
    "    input_data_x_positive_only, output_label_y_positive_only, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb3e2f0-c872-4c6c-a696-344eaf751a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_shap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8cdef3-365f-4081-b848-1649400c2f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe390163-fa8f-4705-9401-7a4cf140326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_features_positive_only[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e78193-fa63-4ec8-9198-bbaf59a9f4d2",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ba69bb-db28-48f2-9e41-fc5cebb0beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, features, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (array-like): Input image features (e.g., images as numpy arrays).\n",
    "            labels (array-like): Corresponding labels for the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Ensure the image has the correct shape\n",
    "        if image.ndim == 2:  # Grayscale image (height, width)\n",
    "            image = np.expand_dims(image, axis=-1)  # Convert to (height, width, 1)\n",
    "        elif image.ndim == 3 and image.shape[2] == 1:  # If the image is (height, width, 1)\n",
    "            image = np.squeeze(image, axis=-1)  # Convert to (height, width)\n",
    "\n",
    "        # Convert to PIL image\n",
    "        if isinstance(image, np.ndarray):\n",
    "            if image.ndim == 2:  # Grayscale\n",
    "                image = Image.fromarray(image)  # Convert from numpy array to PIL Image\n",
    "            elif image.ndim == 3:  # RGB\n",
    "                image = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82412520-e794-4585-b0ba-d9a481fbb5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Custom normalization for grayscale images (mean and std for single channel)\n",
    "# normalize_grayscale = transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     normalize_grayscale,  # Apply normalization only for grayscale images\n",
    "# ])\n",
    "\n",
    "# Use a conditional normalization based on the number of channels\n",
    "def dynamic_normalize(tensor):\n",
    "    \"\"\"\n",
    "    Normalize tensors dynamically based on the number of channels.\n",
    "    Args:\n",
    "        tensor: A PyTorch tensor representing the image.\n",
    "    Returns:\n",
    "        Normalized tensor.\n",
    "    \"\"\"\n",
    "    if tensor.shape[0] == 3:  # RGB Image\n",
    "        return transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(tensor)\n",
    "    elif tensor.shape[0] == 1:  # Grayscale Image\n",
    "        return transforms.Normalize(mean=[0.485], std=[0.229])(tensor)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported number of channels: {tensor.shape[0]}\")\n",
    "\n",
    "# Add this dynamic normalization into your transform chain:\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    dynamic_normalize,  # Apply normalization dynamically based on channels\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9418fcf5-f8ea-4538-8324-9f895f45ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = CustomImageDataset(features=train_features, labels=train_labels, transform=transform)\n",
    "# #test_dataset = CustomImageDataset(features=test_features, labels=test_labels, transform=transform)\n",
    "\n",
    "# # Create DataLoader instances\n",
    "# loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# #test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0faec4f-84f6-43f5-9f50-d2ff2c5e4832",
   "metadata": {},
   "source": [
    "### Load datset for NonCOVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c74894-d8d9-468c-a8d0-2264ab66f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_negative = CustomImageDataset(features=train_features_negative_only, labels=train_labels_negative_only, transform=transform)\n",
    "#test_dataset = CustomImageDataset(features=test_features, labels=test_labels, transform=transform)\n",
    "\n",
    "# Create DataLoader instances\n",
    "loader_negative = DataLoader(train_dataset_negative, batch_size=64, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd220a6-a04f-4f19-8337-a9cf6ffab399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfa8e980-38ae-4b89-b8f1-dd4c1938ea1f",
   "metadata": {},
   "source": [
    "### Load datset for COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00330b6-11f8-49e6-89af-fa7e505d31df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_positive = CustomImageDataset(features=train_features_positive_only, labels=train_labels_positive_only, transform=transform)\n",
    "#test_dataset = CustomImageDataset(features=test_features, labels=test_labels, transform=transform)\n",
    "\n",
    "# Create DataLoader instances\n",
    "loader_positive = DataLoader(train_dataset_positive, batch_size=64, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313064bd-0ba2-437a-b6b4-2c8537a9a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_model = DiffusionModel(input_size=300*300).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5535afce-3183-4623-bb44-0e775e325896",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f011e9-ad43-4c1a-a583-9f1b4e60b1b2",
   "metadata": {},
   "source": [
    "### Building the multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca88b0-b2f3-404e-8048-81a3b80fec45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# U-Net based Diffusion Model (Generator)\n",
    "class DiffusionGenerator(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=3):\n",
    "        super(DiffusionGenerator, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, output_channels, kernel_size=2, stride=2),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# GAN Generator\n",
    "class GANGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_size):\n",
    "        super(GANGenerator, self).__init__()\n",
    "        self.init_size = img_size // 4\n",
    "        self.fc = nn.Linear(latent_dim, 128 * self.init_size ** 2)\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.fc(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "# GAN Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * (img_size // 8) * (img_size // 8), 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        validity = self.model(img)\n",
    "        return validity\n",
    "\n",
    "# Training Loop for the Multimodal Model\n",
    "def train_multimodal_model(diffusion_generator, gan_generator, discriminator, dataloader, gen_optimizer, disc_optimizer, num_epochs, latent_dim):\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_images, _ in tqdm(dataloader):\n",
    "            batch_size = real_images.size(0)\n",
    "            valid = torch.ones(batch_size, 1, device=real_images.device)\n",
    "            fake = torch.zeros(batch_size, 1, device=real_images.device)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            disc_optimizer.zero_grad()\n",
    "            real_images = real_images.to(real_images.device)\n",
    "            real_loss = adversarial_loss(discriminator(real_images), valid)\n",
    "\n",
    "            # Generate fake images using GAN Generator\n",
    "            z = torch.randn(batch_size, latent_dim, device=real_images.device)\n",
    "            fake_images_gan = gan_generator(z)\n",
    "            fake_loss = adversarial_loss(discriminator(fake_images_gan.detach()), fake)\n",
    "            disc_loss = (real_loss + fake_loss) / 2\n",
    "            disc_loss.backward()\n",
    "            disc_optimizer.step()\n",
    "\n",
    "            # Train GAN Generator\n",
    "            gen_optimizer.zero_grad()\n",
    "            gen_loss = adversarial_loss(discriminator(fake_images_gan), valid)\n",
    "            gen_loss.backward()\n",
    "            gen_optimizer.step()\n",
    "\n",
    "            # Train Diffusion Generator (Optional step for separate diffusion training)\n",
    "            diffusion_optimizer = optim.Adam(diffusion_generator.parameters(), lr=1e-4)\n",
    "            diffusion_optimizer.zero_grad()\n",
    "            fake_images_diffusion = diffusion_generator(real_images)\n",
    "            diffusion_loss = nn.MSELoss()(fake_images_diffusion, real_images)\n",
    "            diffusion_loss.backward()\n",
    "            diffusion_optimizer.step()\n",
    "\n",
    "# Initialize models and optimizers\n",
    "img_size = 64\n",
    "latent_dim = 100\n",
    "num_epochs = 50\n",
    "\n",
    "diffusion_generator = DiffusionGenerator().to(\"cuda\")\n",
    "gan_generator = GANGenerator(latent_dim, img_size).to(\"cuda\")\n",
    "discriminator = Discriminator(img_size).to(\"cuda\")\n",
    "\n",
    "gen_optimizer = optim.Adam(gan_generator.parameters(), lr=1e-4)\n",
    "disc_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "train_multimodal_model(diffusion_generator, gan_generator, discriminator, dataloader, gen_optimizer, disc_optimizer, num_epochs, latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688cb5f1-7611-421c-94dc-d9bb21e54e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dd46e1-0a16-4245-a737-6d6603e126df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
