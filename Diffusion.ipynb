{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea1b12-93d5-4ad0-90c7-7fd0460ea161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd88084-324f-4378-bebf-e54de39aa5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os.path\n",
    "from PIL import Image\n",
    "from skimage.io import imread, imsave, imshow, show, imread_collection, imshow_collection\n",
    "import os, logging\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'  \n",
    "from os import listdir\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "#import keras_tuner as kt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tensorflow import keras\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "os.environ[\"TF_CPP_VMODULE\"]=\"gpu_process_state=10,gpu_cudamallocasync_allocator=10\"\n",
    "a = tf.zeros([], tf.float32)\n",
    "## Imports libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078478b-7170-4af0-b764-f5e2480c692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07cc7c-34de-4259-a6cb-b0d560369db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renameImageFiles(folderpath, prefix,fileExtension):\n",
    "    folder_path = folderpath\n",
    "    new_prefix = prefix\n",
    "\n",
    "    for i, file_path in enumerate(glob.glob(folder_path + '*.'+fileExtension)):\n",
    "        new_file_name = new_prefix + '_' + str(i+1) + '.'+fileExtension\n",
    "        os.rename(file_path, os.path.join(folder_path, new_file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc314d8-3523-4c5c-83c0-e0c5ef3b3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/home/maxwellsam/Multimodal-COVID19-Images/dataset/CT_COVID/'\n",
    "prefix1 = 'ct_covid'\n",
    "path2 = '/home/maxwellsam/Multimodal-COVID19-Images/dataset/CT_NonCOVID/'\n",
    "prefix2 = 'ct_noncovid'\n",
    "\n",
    "# renameImageFiles(path1, prefix1,'png')\n",
    "# renameImageFiles(path2, prefix2,'png')\n",
    "# renameImageFiles(path2, prefix2,'jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a99cd28-c5b5-422b-a3c4-79daeecc2ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImages(imgDirPath, binary_label):\n",
    "    img_names = list()\n",
    "    try:\n",
    "        with os.scandir(imgDirPath) as dirs:\n",
    "            for entry in dirs:\n",
    "                img_names.append(entry.name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scanning directory: {e}\")\n",
    "        return None\n",
    "\n",
    "    all_features = []\n",
    "    for img in img_names:\n",
    "        try:\n",
    "            path = imgDirPath + img\n",
    "            cv_img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if cv_img is None:\n",
    "                print(f\"Error reading image: {path}\")\n",
    "                continue\n",
    "\n",
    "            cv_img2 = cv2.resize(cv_img, (300, 300), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            nFeatures = (cv_img2.shape[0] * cv_img2.shape[1])\n",
    "            features = np.reshape(cv_img2, nFeatures)\n",
    "            all_features.append(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img}: {e}\")\n",
    "\n",
    "    if len(all_features) == 0:\n",
    "        print(\"No valid images found.\")\n",
    "        return None\n",
    "\n",
    "    imgs_df = pd.DataFrame(np.array(all_features), index=img_names)\n",
    "    if binary_label == 0:\n",
    "        imgs_df['class_label'] = np.zeros((imgs_df.shape[0]), dtype=int)\n",
    "    else:\n",
    "        imgs_df['class_label'] = np.ones((imgs_df.shape[0]), dtype=int)\n",
    "\n",
    "    return imgs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bc82c6-c44d-47f9-bde6-ff4027a2a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df =  processImages(path1,1)#1--> covid-19 positive\n",
    "ct_noncovid_features_df =  processImages(path2,0)#0 ---> covnid-19 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ed399-eb84-4984-8087-52031fa1664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvd_imgs = [ct_noncovid_features_df, ct_covid_features_df]\n",
    "cvd_imgs_dataset = pd.concat(cvd_imgs)\n",
    "for i in range(100):\n",
    "    # shuffle the DataFrame rows\n",
    "    cvd_imgs_dataset = cvd_imgs_dataset.sample(frac = 1)\n",
    "# cvd_imgs_dataset_colour = cv2.cvtColor(cvd_imgs_dataset, cv2.COLOR_BGR2RGB)\n",
    "display(cvd_imgs_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c53e333-dc48-47dc-8eaf-88e0d6eb4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_noncovid_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5212b76c-4ba0-47b4-aa9e-06ea0846c9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_covid_features_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5042e05-3ad4-4c52-9e6d-2beba1529a9d",
   "metadata": {},
   "source": [
    "#### Prepare negative covid images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0553ca-9648-46ba-a516-cc6450e2880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x_ = ct_noncovid_features_df.iloc[:,:-1].to_numpy().reshape((2173,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "ct_noncovid_features_df['output_encode'] = label_encoder.fit_transform(ct_noncovid_features_df['class_label'])\n",
    "ct_noncovid_features_df\n",
    "ct_noncovid_features_df = pd.get_dummies(ct_noncovid_features_df, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y_ = np.array(ct_noncovid_features_df[['class_label']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "input_data_x_negative_only = input_data_x_\n",
    "output_label_y_negative_only = output_label_y_\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x_negative_only.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y_negative_only.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea13fb-9d18-49ca-aa97-fbf08aed4a79",
   "metadata": {},
   "source": [
    "#### Prepare positive covid images for machine learning ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d2452-b1a2-4f72-8828-c257238acbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_x__ = ct_covid_features_df.iloc[:,:-1].to_numpy().reshape((2476,300,300,1))\n",
    "#input_data_x = cvd_imgs_dataset.iloc[:,:-1].to_numpy()\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "ct_covid_features_df['output_encode'] = label_encoder.fit_transform(ct_covid_features_df['class_label'])\n",
    "ct_covid_features_df\n",
    "ct_covid_features_df = pd.get_dummies(ct_covid_features_df, columns =['output_encode'])\n",
    "##Getting the input_labels and input_features for training and testing model\n",
    "output_label_y__ = np.array(ct_covid_features_df[['class_label']])\n",
    "# print('Input_x Data: \\n{0}'.format(input_data_x))\n",
    "# print('Output_y Data: \\n{0}'.format(output_label_y))\n",
    "input_data_x_positive_only = input_data_x__\n",
    "output_label_y_positive_only = output_label_y__\n",
    "print('Input_x Data Shape: \\n{0}'.format(input_data_x_positive_only.shape))\n",
    "print('Output_y Data Shape: \\n{0}'.format(output_label_y_positive_only.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6d361-36f3-456b-b9f3-8abd22ee24f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_negative_only, test_features_negative_only, train_labels_negative_only, test_labels_negative_only = train_test_split(\n",
    "    input_data_x_negative_only, output_label_y_negative_only, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f791f28-f645-4245-ad1d-af6af811cab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_positive_only, test_features_positive_only, train_labels_positive_only, test_labels__positive_only = train_test_split(\n",
    "    input_data_x_positive_only, output_label_y_positive_only, test_size=.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c85be-730c-4440-a870-eb5db868cfc3",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa597f6-36fd-439d-8916-6b48ca5735f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, features, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (array-like): Input image features (e.g., images as numpy arrays).\n",
    "            labels (array-like): Corresponding labels for the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Ensure the image has the correct shape\n",
    "        if image.ndim == 2:  # Grayscale image (height, width)\n",
    "            image = np.expand_dims(image, axis=-1)  # Convert to (height, width, 1)\n",
    "        elif image.ndim == 3 and image.shape[2] == 1:  # If the image is (height, width, 1)\n",
    "            image = np.squeeze(image, axis=-1)  # Convert to (height, width)\n",
    "\n",
    "        # Convert to PIL image\n",
    "        if isinstance(image, np.ndarray):\n",
    "            if image.ndim == 2:  # Grayscale\n",
    "                image = Image.fromarray(image)  # Convert from numpy array to PIL Image\n",
    "            elif image.ndim == 3:  # RGB\n",
    "                image = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32264260-6086-4288-a486-ead91f9f8fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Custom normalization for grayscale images (mean and std for single channel)\n",
    "# normalize_grayscale = transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     normalize_grayscale,  # Apply normalization only for grayscale images\n",
    "# ])\n",
    "\n",
    "# Use a conditional normalization based on the number of channels\n",
    "def dynamic_normalize(tensor):\n",
    "    \"\"\"\n",
    "    Normalize tensors dynamically based on the number of channels.\n",
    "    Args:\n",
    "        tensor: A PyTorch tensor representing the image.\n",
    "    Returns:\n",
    "        Normalized tensor.\n",
    "    \"\"\"\n",
    "    if tensor.shape[0] == 3:  # RGB Image\n",
    "        return transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(tensor)\n",
    "    elif tensor.shape[0] == 1:  # Grayscale Image\n",
    "        return transforms.Normalize(mean=[0.485], std=[0.229])(tensor)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported number of channels: {tensor.shape[0]}\")\n",
    "\n",
    "# Add this dynamic normalization into your transform chain:\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    dynamic_normalize,  # Apply normalization dynamically based on channels\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4f0bcc-ae89-4335-80a2-215b3a0201bb",
   "metadata": {},
   "source": [
    "### Load datset for NonCOVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ad5b0-f43b-41bf-80c0-707040d849cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_negative = CustomImageDataset(features=train_features_negative_only, labels=train_labels_negative_only, transform=transform)\n",
    "#test_dataset = CustomImageDataset(features=test_features, labels=test_labels, transform=transform)\n",
    "\n",
    "# Create DataLoader instances\n",
    "loader_negative = DataLoader(train_dataset_negative, batch_size=64, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea20e8-7774-4c20-99de-8bcdb4d3d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(loader_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76880e08-2176-440e-b45e-7e0e612ede7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader_negative:\n",
    "    images, labels = batch \n",
    "    \n",
    "    # Convert images from PyTorch tensor to NumPy array\n",
    "    images_np = images.numpy()\n",
    "    \n",
    "    # Convert the NumPy array to TensorFlow tensor\n",
    "    images_tf = tf.convert_to_tensor(images_np, dtype=tf.float32)\n",
    "    \n",
    "print(images_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed4ab71-865f-4a7d-85a4-c500d0b86622",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_images = images_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91924d6-4237-4e19-9d68-63b22e924a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb4d322-a174-4fde-8159-f0e0d40179bb",
   "metadata": {},
   "source": [
    "### Load datset for COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e635f-422d-4fff-bb48-c55f924ad7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_positive = CustomImageDataset(features=train_features_positive_only, labels=train_labels_positive_only, transform=transform)\n",
    "#test_dataset = CustomImageDataset(features=test_features, labels=test_labels, transform=transform)\n",
    "\n",
    "# Create DataLoader instances\n",
    "loader_positive = DataLoader(train_dataset_positive, batch_size=64, shuffle=True)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b719b510-a986-4b5f-91e6-94132c150989",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader_positive:\n",
    "    images_, labels_ = batch \n",
    "    \n",
    "    # Convert images from PyTorch tensor to NumPy array\n",
    "    images_np_ = images.numpy()\n",
    "    #print(images_np_.shape)\n",
    "    # Convert the NumPy array to TensorFlow tensor\n",
    "    images_tf_ = tf.convert_to_tensor(images_np, dtype=tf.float32)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26e0f3e-a257-4029-bcdc-04d32fe039cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_images = images_tf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca4249a-8050-42c9-9a95-80a4fd619ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(positive_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6cc12-e283-4804-aaeb-8841e8340002",
   "metadata": {},
   "source": [
    "#### GAN-VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f1796-69bd-452f-a86f-26d0859ea653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels=1, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, hidden_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim * 2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim * 2, hidden_dim * 4, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(hidden_dim * 2, hidden_dim, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, input_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class DiffusionModel:\n",
    "    def __init__(self, model, timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        self.model = model\n",
    "        self.timesteps = timesteps\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "    \n",
    "    def forward_diffusion(self, x0, t):\n",
    "        noise = torch.randn_like(x0)\n",
    "        sqrt_alpha_bar = torch.sqrt(self.alpha_bars[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bars[t])[:, None, None, None]\n",
    "        return sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise, noise\n",
    "\n",
    "    def sample(self, shape, device):\n",
    "        x = torch.randn(shape, device=device)\n",
    "        for t in reversed(range(self.timesteps)):\n",
    "            t_tensor = torch.tensor([t], device=device)\n",
    "            predicted_noise = self.model(x)\n",
    "            alpha_bar = self.alpha_bars[t]\n",
    "            alpha = self.alphas[t]\n",
    "            beta = self.betas[t]\n",
    "            x = (1 / torch.sqrt(alpha)) * (x - (beta / torch.sqrt(1 - alpha_bar)) * predicted_noise)\n",
    "            if t > 0:\n",
    "                x += torch.sqrt(beta) * torch.randn_like(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dc61a2-38c8-4635-a0b5-abc13bd474a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56211bd6-9184-49a3-8e04-526ba9fb3f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels=1, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, hidden_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim * 2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim * 2, hidden_dim * 4, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(hidden_dim * 2, hidden_dim, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, input_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class DiffusionModel:\n",
    "    def __init__(self, model, timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        self.model = model\n",
    "        self.timesteps = timesteps\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "    \n",
    "    def forward_diffusion(self, x0, t):\n",
    "        noise = torch.randn_like(x0)\n",
    "        sqrt_alpha_bar = torch.sqrt(self.alpha_bars[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bars[t])[:, None, None, None]\n",
    "        return sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise, noise\n",
    "\n",
    "    def sample(self, shape, device):\n",
    "        x = torch.randn(shape, device=device)\n",
    "        for t in reversed(range(self.timesteps)):\n",
    "            t_tensor = torch.tensor([t], device=device)\n",
    "            predicted_noise = self.model(x)\n",
    "            alpha_bar = self.alpha_bars[t]\n",
    "            alpha = self.alphas[t]\n",
    "            beta = self.betas[t]\n",
    "            x = (1 / torch.sqrt(alpha)) * (x - (beta / torch.sqrt(1 - alpha_bar)) * predicted_noise)\n",
    "            if t > 0:\n",
    "                x += torch.sqrt(beta) * torch.randn_like(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18847c74-4462-4c47-b4d0-916d623a7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train(negative_images, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de1728-4ea0-4c19-83b2-b5ca823aae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(negative_images.shape)\n",
    "image_____ = tf.transpose(negative_images, perm=[0, 2, 3, 1])\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bcd3e4-7f98-4a48-b5b4-2e5f1bf13e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.io import imshow, show\n",
    "\n",
    "# Ensure the image is a NumPy array with a proper dtype (e.g., uint8 for image data)\n",
    "for batch in negative_images:\n",
    "    for i in range(1):\n",
    "        imgd = batch[i].numpy()  # Convert tensor to NumPy array\n",
    "        img = np.clip(imgd, 0, 255).astype(np.uint8)  # Ensure the values are within the expected range\n",
    "        imshow(img)\n",
    "        show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f66533-f5f7-4825-904a-f821609bb172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a23308-4c1b-4ebc-befb-c786bc421da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f939d7-1fe4-4260-904d-b5deb8ec9b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
